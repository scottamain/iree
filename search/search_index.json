{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IREE","text":"<p>IREE (Intermediate Representation Execution Environment1) is an MLIR-based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> Ahead-of-time compilation of scheduling and execution logic together</li> <li> Support for dynamic shapes, flow control, streaming, and other advanced       model features</li> <li> Optimized for many CPU and GPU architectures</li> <li> Low overhead, pipelined execution for efficient power and resource usage</li> <li> Binary size as low as 30KB on embedded systems</li> <li> Debugging and profiling support</li> </ul>"},{"location":"#support-matrix","title":"Support matrix","text":"<p>IREE supports importing from a variety of ML frameworks:</p> <ul> <li> TensorFlow</li> <li> TensorFlow Lite</li> <li> JAX</li> <li> PyTorch</li> <li> ONNX (hoped for)</li> </ul> <p>The IREE compiler tools run on  Linux,  Windows, and  macOS and can generate efficient code for a variety of runtime platforms:</p> <ul> <li> Linux</li> <li> Windows</li> <li> Android</li> <li> macOS</li> <li> iOS</li> <li> Bare metal</li> <li> WebAssembly (planned)</li> </ul> <p>and architectures:</p> <ul> <li> ARM</li> <li> x86</li> <li> RISC-V</li> </ul> <p>Support for hardware accelerators and APIs is also included:</p> <ul> <li> Vulkan</li> <li> CUDA</li> <li> Metal (planned)</li> <li> WebGPU (planned)</li> </ul>"},{"location":"#project-architecture","title":"Project architecture","text":"<p>IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan, and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V.</p> <p> </p>"},{"location":"#workflow-overview","title":"Workflow overview","text":"<p>Specific examples outlining IREE's workflow can be found in the User Getting Started Guide. Using IREE involves the following general steps:</p> <ol> <li> <p>Import your model</p> <p>Develop your program using one of the supported frameworks, then run your model using one of IREE's import tools.</p> </li> <li> <p>Select your deployment configuration</p> <p>Identify your target platform, accelerator(s), and other constraints.</p> </li> <li> <p>Compile your model</p> <p>Compile through IREE, picking compilation targets based on your deployment configuration.</p> </li> <li> <p>Run your model</p> <p>Use IREE's runtime components to execute your compiled model.</p> </li> </ol>"},{"location":"#importing-models-from-ml-frameworks","title":"Importing models from ML frameworks","text":"<p>IREE supports importing models from a growing list of ML frameworks and model formats:</p> <ul> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>JAX</li> <li>PyTorch</li> </ul>"},{"location":"#selecting-deployment-configurations","title":"Selecting deployment configurations","text":"<p>IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators.</p> <ul> <li>What platforms are you targeting? Desktop? Mobile? An embedded system?</li> <li>What hardware should the bulk of your model run on? CPU? GPU?</li> <li>How fixed is your model itself? Can the weights be changed? Do you want   to support loading different model architectures dynamically?</li> </ul> <p>IREE supports the full set of these configurations using the same underlying technology.</p>"},{"location":"#compiling-models","title":"Compiling models","text":"<p>Model compilation is performed ahead-of-time on a host machine for any combination of targets. The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic.</p> <p>For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution, native code with static or dynamic linkage and the associated function calls are generated.</p>"},{"location":"#running-models","title":"Running models","text":"<p>IREE offers a low level C API, as well as several specialized sets of bindings for running IREE models using other languages:</p> <ul> <li>C API</li> <li>Python</li> <li>TensorFlow Lite</li> </ul>"},{"location":"#communication-channels","title":"Communication channels","text":"<ul> <li> GitHub issues: Feature requests,     bugs, and other work tracking</li> <li> IREE Discord server: Daily development     discussions with the core team and collaborators</li> <li> iree-discuss email list:     Announcements, general and low-priority discussion</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<p>IREE is in the early stages of development and is not yet ready for broad adoption. Check out the long-term design roadmap to get a sense of where we're headed.</p> <p>We plan on a quarterly basis using OKRs. Review our latest objectives to see what we're up to.</p> <p>We use GitHub Projects to track progress on IREE components and specific efforts and GitHub Milestones to track the work associated with plans for each quarter.</p> <ol> <li> <p>Pronounced \"eerie\" and often styled with the   emoji\u00a0\u21a9</p> </li> </ol>"},{"location":"bindings/","title":"Bindings","text":"<p>IREE offers specialized sets of bindings for running compiled programs from various languages or with specific APIs:</p> <ul> <li>Runtime C API</li> <li>Compiler and runtime Python bindings</li> <li>Runtime TensorFlow Lite bindings</li> </ul>"},{"location":"bindings/c-api/","title":"C API bindings","text":"<p>IREE provides a low level C API for its runtime1, which can be used directly or through higher level APIs and language bindings built on top of it.</p> <p>API header files are organized by runtime component:</p> Component header file Overview iree/base/api.h Core API, type definitions, ownership policies, utilities iree/vm/api.h VM APIs: loading modules, I/O, calling functions iree/hal/api.h HAL APIs: device management, synchronization, accessing hardware features <p>The samples/ directory demonstrates several ways to use IREE's C API.</p>"},{"location":"bindings/c-api/#prerequisites","title":"Prerequisites","text":"<p>To use IREE's C API, you will need to build the runtime from source. The iree-template-cpp community project also shows how to integrate IREE into an external project using CMake2.</p>"},{"location":"bindings/c-api/#concepts","title":"Concepts","text":"<p>By default, IREE uses its own tiny Virtual Machine (VM) at runtime to interpret program instructions on the host system. VM instructions may also be lowered further to LLVM IR, C, or other representations for static or resource constrained deployment.</p> <p>The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. It builds streams of more complex program logic and dense math into command buffers that are dispatched to hardware backends through the Hardware Abstraction Layer (HAL) interface.</p> <p>Most interaction with IREE's C API involves either the VM or the HAL.</p>"},{"location":"bindings/c-api/#iree-vm","title":"IREE VM","text":"<ul> <li>VM instances can serve multiple isolated execution contexts</li> <li>VM contexts are effectively sandboxes for loading modules and running   programs</li> <li>VM modules provide extra functionality to execution contexts, such as   access to hardware accelerators through the HAL. Compiled user programs are   also modules.</li> </ul>"},{"location":"bindings/c-api/#iree-hal","title":"IREE HAL","text":"<ul> <li>HAL drivers are used to enumerate and create HAL devices</li> <li>HAL devices interface with hardware, such as by allocating device memory,   preparing executables, recording and dispatching command buffers, and   synchronizing with the host</li> <li>HAL buffers and buffer views represent storage and shaped/typed views   into that storage (aka \"tensors\")</li> </ul>"},{"location":"bindings/c-api/#using-the-c-api","title":"Using the C API","text":""},{"location":"bindings/c-api/#setup","title":"Setup","text":"<p>Include headers:</p> <pre><code>#include \"iree/base/api.h\"\n#include \"iree/hal/api.h\"\n#include \"iree/vm/api.h\"\n\n// The VM bytecode and HAL modules will typically be included, along\n// with those for the specific HAL drivers your application uses.\n// Functionality extensions can be used via custom modules.\n#include \"iree/modules/hal/module.h\"\n#include \"iree/hal/drivers/local_task/registration/driver_module.h\"\n#include \"iree/vm/bytecode_module.h\"\n</code></pre> <p>Check the API version and register components:</p> <pre><code>// Device drivers are managed through registries.\n// Applications may use multiple registries to more finely control driver\n// lifetimes and visibility.\nIREE_CHECK_OK(iree_hal_local_task_driver_module_register(\niree_hal_driver_registry_default()));\n</code></pre> <p>Tip</p> <p>The <code>IREE_CHECK_OK()</code> macro calls <code>assert()</code> if an error occurs. Applications should propagate errors and handle or report them as desired.</p>"},{"location":"bindings/c-api/#configure-stateful-objects","title":"Configure stateful objects","text":"<p>Create a VM instance along with a HAL driver and device:</p> <pre><code>// Applications should try to reuse instances so resource usage across contexts\n// is handled and extraneous device interaction is avoided.\niree_vm_instance_t* instance = NULL;\nIREE_CHECK_OK(iree_vm_instance_create(iree_allocator_system(), &amp;instance));\n\n// Modules with custom types must be statically registered before use.\nIREE_CHECK_OK(iree_hal_module_register_all_types(instance));\n\n// We use the CPU \"local-task\" driver in this example, but could use a different\n// driver like the GPU \"vulkan\" driver. The driver(s) used should match with\n// the target(s) specified during compilation.\niree_hal_driver_t* driver = NULL;\nIREE_CHECK_OK(iree_hal_driver_registry_try_create(\niree_hal_driver_registry_default(),\niree_string_view_literal(\"local-task\"),\niree_allocator_system(), &amp;driver));\n\n// Drivers may support multiple devices, such as when a machine has multiple\n// GPUs. You may either enumerate devices and select based on their properties,\n// or just use the default device.\niree_hal_device_t* device = NULL;\nIREE_CHECK_OK(iree_hal_driver_create_default_device(\ndriver, iree_allocator_system(), &amp;device));\n\n// Create a HAL module initialized to use the newly created device.\n// We'll load this module into a VM context later.\niree_vm_module_t* hal_module = NULL;\nIREE_CHECK_OK(\niree_hal_module_create(instance, device, IREE_HAL_MODULE_FLAG_NONE,\niree_allocator_system(), &amp;hal_module));\n// The reference to the driver can be released now.\niree_hal_driver_release(driver);\n</code></pre> <p>Tip</p> <p>The default <code>iree_allocator_system()</code> is backed by <code>malloc</code> and <code>free</code>, but custom allocators may also be used.</p> <p>Load a vmfb bytecode module containing program data:</p> <pre><code>// (Application-specific loading into memory, such as loading from a file)\n\niree_vm_module_t* bytecode_module = NULL;\nIREE_CHECK_OK(iree_vm_bytecode_module_create(\ninstance,\niree_const_byte_span_t{module_data, module_size},\n/*flatbuffer_allocator=*/iree_allocator_null(),\n/*allocator=*/iree_allocator_system(), &amp;bytecode_module));\n</code></pre> <p>Note</p> <p>Many IREE samples use <code>c_embed_data</code> to embed vmfb files as C code to avoid file I/O and ease portability. Applications should use what makes sense for their platforms and deployment configurations.</p> <p>Create a VM context and load modules into it:</p> <pre><code>iree_vm_context_t* context = NULL;\niree_vm_module_t* modules[2] = {hal_module, bytecode_module};\nIREE_CHECK_OK(iree_vm_context_create_with_modules(\ninstance, IREE_VM_CONTEXT_FLAG_NONE,\nIREE_ARRAYSIZE(modules), modules,\niree_allocator_system(), &amp;context));\n// References to the modules can be released now.\niree_vm_module_release(hal_module);\niree_vm_module_release(bytecode_module);\n</code></pre> <p>Look up the function(s) to call:</p> <pre><code>iree_vm_function_t main_function;\nIREE_CHECK_OK(iree_vm_context_resolve_function(\ncontext, iree_string_view_literal(\"module.main_function\"), &amp;main_function));\n</code></pre>"},{"location":"bindings/c-api/#invoke-functions","title":"Invoke functions","text":"<pre><code>// (Application-specific I/O buffer setup, making data available to the device)\n\nIREE_CHECK_OK(iree_vm_invoke(context, main_function, IREE_VM_INVOCATION_FLAG_NONE,\n/*policy=*/NULL, inputs, outputs,\niree_allocator_system()));\n\n// (Application-specific output buffer retrieval and reading back from the device)\n</code></pre>"},{"location":"bindings/c-api/#cleanup-resources","title":"Cleanup resources","text":"<pre><code>iree_hal_device_release(device);\niree_vm_context_release(context);\niree_vm_instance_release(instance);\n</code></pre> <ol> <li> <p>We are exploring adding a C API for IREE's compiler, see   this GitHub issue \u21a9</p> </li> <li> <p>We plan on deploying via vcpkg in the   future too, see   this GitHub project \u21a9</p> </li> </ol>"},{"location":"bindings/python/","title":"Python bindings","text":"<p>Info</p> <p>API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs.</p>"},{"location":"bindings/python/#overview","title":"Overview","text":"<p>IREE offers Python bindings split into several packages, covering different components:</p> PIP package name Description <code>iree-compiler</code> IREE's generic compiler tools and helpers <code>iree-runtime</code> IREE's runtime, including CPU and GPU backends <code>iree-tools-tf</code> Tools for importing from TensorFlow <code>iree-tools-tflite</code> Tools for importing from TensorFlow Lite <code>iree-tools-xla</code> Tools for importing from XLA <code>iree-jax</code> Tools for importing from JAX <p>Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends.</p> <p>Warning</p> <p>The TensorFlow, TensorFlow Lite, and XLA packages are currently only available on Linux and macOS. They are not available on Windows yet (see this issue).</p>"},{"location":"bindings/python/#prerequisites","title":"Prerequisites","text":"<p>To use IREE's Python bindings, you will first need to install Python 3 and pip, as needed.</p> Tip <p>We recommend using virtual environments to manage python packages, such as through <code>venv</code> (about, tutorial):</p> Linux and MacOSWindows <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\n.venv\\Scripts\\activate.bat\n</code></pre> <p>When done, run <code>deactivate</code>.</p> <p>Next, install packages:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install numpy absl-py\n</code></pre>"},{"location":"bindings/python/#installing-iree-packages","title":"Installing IREE packages","text":""},{"location":"bindings/python/#prebuilt-packages","title":"Prebuilt packages","text":"<p>Stable release packages are published to PyPI.</p> MinimalAll packages <p>To install just the core IREE packages:</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime\n</code></pre> <p>To install IREE packages with tools for all frontends:</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime \\\niree-tools-tf \\\niree-tools-tflite \\\niree-tools-xla\n</code></pre> <p>Tip</p> <p>Nightly packages are also published on GitHub releases. To use these, run <code>pip install</code> with this extra option:</p> <pre><code>--find-links https://iree-org.github.io/iree/pip-release-links.html\n</code></pre>"},{"location":"bindings/python/#building-from-source","title":"Building from source","text":"<p>See Building Python bindings page for instructions for building from source.</p>"},{"location":"bindings/python/#using-the-python-bindings","title":"Using the Python bindings","text":"<p>API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs.</p> <p>Check out the samples in IREE's samples/colab/ directory and the iree-samples repository for examples using the Python APIs.</p>"},{"location":"bindings/tensorflow-lite/","title":"TensorFlow Lite bindings","text":"<p>Todo</p> <p>Issue#5462: write this documentation</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#latest-posts-from-the-iree-team","title":"Latest posts from the IREE team","text":"<ul> <li>2021-10-15: CUDA backend</li> <li>2021-10-13: Work in progress on Matrix Multiplication on CPU</li> <li>2021-07-19: TFLite Support via TOSA</li> </ul>"},{"location":"blog/2021-07-19-tflite-tosa/","title":"TFLite Support via TOSA","text":"<p>Monday, July 19, 2021  By Rob Suderman and Jenni Kilduff</p>"},{"location":"blog/2021-07-19-tflite-tosa/#tflite-support-via-tosa","title":"TFLite Support via TOSA","text":"<p>IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA, an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite FlatBuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends.</p> <p></p> <p>Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment.</p> <p>Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert. More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support.</p>"},{"location":"blog/2021-07-19-tflite-tosa/#examples","title":"Examples","text":"<p>TFLite with IREE is available in Python and Java.  We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a FlatBuffer and run using IREE.  We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR.  More information on IREE\u2019s TFLite frontend is available here.</p>"},{"location":"blog/2021-10-13-mmt4d/","title":"Work in progress on Matrix Multiplication on CPU","text":"<p>Wednesday, October 13, 2021 By Ahmed Taei, Benoit Jacob</p>"},{"location":"blog/2021-10-13-mmt4d/#work-in-progress-on-matrix-multiplication-on-cpu","title":"Work in progress on Matrix Multiplication on CPU","text":""},{"location":"blog/2021-10-13-mmt4d/#introduction","title":"Introduction","text":"<p>Matrix multiplication (matmul) is an important operation in ML workloads that poses specific challenges to code generation. For example, matmul makes repeated accesses to the same data, which makes locality of reference a top concern.</p> <p>Moreover, modern CPUs instruction set architectures (ISAs) offer specialized SIMD instructions that the matmul implementation needs to use to achieve optimal performance, and these instructions expect data to be in a particular layout.</p> <p>This article is about an in-development MLIR operation, <code>linalg.mmt4d</code>, offering a compilation path for <code>linalg.matmul</code> that is designed from the ground up for these efficiency considerations.</p> <p>We are still in the early implementation phase of this <code>linalg.mmt4d</code> plan, but we feel confident that we know where we are going because what we are really doing here is importing into the compiler what we have learned working on optimized matrix multiplication libraries, particularly Ruy. We know what loop schedule and kernel we want the compiler to generate \u2014 essentially the same as we wrote in Ruy, give or take additional optimizations such as fusions and constant folding that become possible now that we are doing this within a compiler. This allows us to focus on how we get the compiler to generate that schedule and kernel with purely algebraic transformations that compose and enable further compiler optimizations.</p> <p>At the basis of this work is the extensible op system of the Linalg dialect in the MLIR compiler toolkit. In this case, a general purpose, mixed precision mmt4d op is defined via a high level description directly in the compiler and is then available to both users of the compiler (as a <code>linalg.mmt4d</code> op) or for direct emission via Python based IR construction (i.e. for direct integration into high level frameworks without rebuilding the compiler). The ability to define such new special forms cheaply, and without any systemic framework level cost, is part of the extensibility and composition story that we expect will become increasingly important in development and deployment scenarios in the future, and in this case, it let us spring board off of high quality code generation which was already well integrated and composed well with other features of the compiler.</p>"},{"location":"blog/2021-10-13-mmt4d/#existing-matrix-multplication-code-generation","title":"Existing Matrix Multplication Code Generation","text":"<p>Let us start by discussing IREE\u2019s existing matmul code generation and highlight the issues that <code>mmt4d</code> aims to overcome.</p> <p>The existing approach operates in-place on the source matrices. When we discuss \"tiling\" in this paragraph, we refer exclusively to the traversal \u2014 how these source matrices are traversed by the matmul loop. There is no \"tiled layout\" here, which will be the key difference with <code>mmt4d</code> below.</p> <p>The destination matrix is tiled into workgroups (CPU threads) tiles, then each workgroup tile is tiled to fit some level of CPU cache, and finally each tile is further tiled to fit target architecture registers (e.g. 8x8).</p> <p>That multi-level tiling means that the code works like the following loop nest:</p> <pre><code>def tiled_matmul(A, B, C, tile_m, tile_n, tile_k, tile_m_v, tile_n_v, tile_k_v):\n m = A.shape[0]\n k = A.shape[1]\n n = B.shape[1]\n for m1 in range(0, m, tile_m):\n   for n1 in range(0, n, tile_n):\n     for k1 in range(0, k, tile_k):\n       # First level of tiling views...\n       lhs_tile = A[m1:m1+tile_m, k1:k1+tile_k]\n       rhs_tile = B[k1:k1+tile_k, n1:n1+tile_n]\n       dst_tile = C[m1:m1+tile_m, n1:n1+tile_n]\n       for mv in range(0, tile_m, tile_m_v):\n         for nv in range(0, tile_n, tile_n_v):\n           for kv in range(0, tile_k, tile_k_v):\n             # Register tiling views...\n             lhs_tile_v = lhs_tile[mv:mv+tile_m_v, kv:kv+tile_k_v]\n             rhs_tile_v = rhs_tile[kv:kv+tile_k_v, nv:nv+tile_n_v]\n             # kernel.\n             dst_tile[mv:mv+tile_m_v, nv:nv+tile_n_v] += np.matmul(lhs_tile_v, rhs_tile_v)\n return C\n</code></pre> <p>The two main problems with this approach are:</p> <ul> <li> <p>Overhead to meet SIMD ISA layout requirements: In practice, the kernel     needs to use specific SIMD     instructions to perform the arithmetic. They expect small tiles of the     matrices to be loaded in registers, in a specific layout. If the matrix data     wasn't already stored in memory in such a tiled layout, then the kernel has     to perform such a data rearrangement on the fly, incurring substantial     overhead. For NxN matrix multiplication, the kernel performs     O(N3) work on O(N2) data, so doing that rearrangement     there means O(N3) overhead where O(N2) should have     sufficed, as this could have been done as a pre-processing step on     O(N2) data.</p> </li> <li> <p>Inefficent memory traversal: For efficiency reasons, we always need     <code>tile_m_v&gt;1</code> and <code>tile_n_v&gt;1</code>. That is because the higher these values, the     fewer memory-load instructions are needed overall; and this is also dictated     by the SIMD instructions that we want to use. But that means that the kernel     is accessing simultaneously multiple rows or columns of the left-hand and     right-hand side matrices. And in this existing approach, they are stored in     linear layout, not in a tiled layout, so these accesses are not contiguous     in memory. This is detrimental to memory access performance, meaning the     CPU caches, in multiple ways. One     is that these multiple non-contiguous accesses may alias each other in the     L1 cache because of low     associativity.</p> </li> </ul>"},{"location":"blog/2021-10-13-mmt4d/#matrix-multiplication-operation-with-4d-tiled-operands","title":"Matrix Multiplication Operation With 4D Tiled Operands","text":"<p>For the reasons above, an efficient matmul implementation must reorder data into a tiled layout matching the target SIMD ISA and making the memory access patterns as contiguous as possible.</p> <p>IREE/MLIR defaults to bufferizing all tensors into a \"row-major\" order, meaning that the last-enumerated dimension is the one that is contiguous in memory. As we prefer not to write custom bufferization code, we can't specify an alternative layout for a tensor. Fortunately, it is possible to represent a 2D tiled layout as a 4D layout. For example, <code>tensor&lt;2x2x2x2xf32&gt;</code> can represent a 4x4 matrix made of 2x2 tiles, each of which is 2x2. The row-major layout on <code>tensor&lt;2x2x2x2xf32&gt;</code> makes each 2x2 tile contiguous and row-major, and arranges the 2x2 tiles themselves into a row-major 2x2 layout in the overall 4x4 matrix.</p> <p>Such a row-major-tiled layout is exactly what we need for the left-hand side of a matrix multiplication, because matrix multiplication traverses the left-hand side matrix row by row. But for the right-hand side matrix, we want a column-major-tiled layout. To solve this problem, we decide to implement not matrix multiplication, but matrix-multiplication-by-transposed-right-hand-side which is where the <code>t</code> in the <code>linalg.mmt4d</code> came from. Now such an op is happy with both the left and right-hand sides being row-major-tiled.</p> <p>The following example illustrates that. In these diagrams, each matrix element is rendered its memory offset.</p> <p></p> <p>To compute the 2x2 block in the destination matrix, we will have to load two yellow blocks from LHS, RHS matrices respectively compute their matmul results (i.e. call the kernel), then the two blue blocks, and so on. As we can see, each tile loads data that is not contiguous. It would be better if we rearranged the elements in the following layout:</p> <p></p> <p>Now tiles are stored contiguously in memory and the kernel can simply load them from memory into the registers that will be directly consumed by the SIMD instructions performing the multiplications. Moreover, the kernel is now loading from just two contiguous data streams, a simple memory access pattern which is sure to be efficient (regarding caches, etc) on any reasonable target hardware.</p> <p>We introduce a <code>linalg.mmt4d</code> operation that performs such a matrix multiplication on matrices in a tiled layout represented as 4D tensors. That leaves the question of how to represent, within the linalg dialect, the conversions between ordinary matrices represented as 2D tensors, and these tiled matrices represented as 4D tensors. Moreover, these conversions should be tileable and decompose well. Thankfully, the transformation from 2D to 4D can be written as a reshape followed by a transpose as in the following digram:</p> <p></p> <p>So we can think of the outermost two dimensions of the 4D representations as the tile position in the overall matrix, and the innermost two as the element position within one tile. Hopefully the following Python pseudocode makes it more concrete:</p> <pre><code>def pack_2d_4d(operand, parallel_size, reduction_size):\n i1 = operand.shape[0] // parallel_size # M1\n i2 = parallel_size    # M0\n j1 = operand.shape[1] // reduction_size # K1\n j2 = reduction_size   # K0\n operand_4d = np.reshape(operand, [i1, i2, j1, j2])\n return np.transpose(operand_4d, [0, 2, 1, 3]) # [M1, K1, M0, K0]\n</code></pre> <p>Now the mmt4d operation will follow a structure as the multi level tiling, for simplicity we considered the case here where no L1 tiling is required only first level of distribution to workgroups:</p> <pre><code>def mmt4d(A, B, C, M0, N0, K0):\n M = A.shape[0]\n N = B.shape[1]\n Bt = np.transpose(B, [1, 0])\n A4d = pack_2d_4d(A, M0, K0)\n Bt4d = pack_2d_4d(Bt, N0, K0)\n M1 = A4d.shape[0]\n N1 = Bt4d.shape[0]\n K1 = A4d.shape[1]\n for m1 in range(0, M1):\n   for n1 in range(0, N1):\n     for k1 in range(0, K1):\n       # Tile views that are contiguous in memory.\n       lhs_tile = np.reshape(A4d[m1, k1, :, :], [M0, K0])\n       rhs_tile = np.reshape(Bt4d[n1, k1, :, :], [N0, K0])\n       # Inner kernel.\n       C[m1, n1, :, :] += np.matmul(lhs_tile, np.transpose(rhs_tile, [1, 0]))\n # 4d -&gt; 2D\n C2d = unpack_4d_2d(C)\n return C2d\n</code></pre> <p>The resulting 4D tiled matrix still needs be rearranged back to the original layout as 2D tensor:</p> <pre><code>def unpack_4d_2d(operand):\n i1 = operand.shape[0] # M1\n j1 = operand.shape[1] # N1\n i2 = operand.shape[2] # M0\n j2 = operand.shape[3] # N0\n operand_transposed = operand.transpose([0, 2, 1, 3]) # [M1, M0, N1, N0]\n return operand_transposed.reshape([i1 * i2, j1 * j2]) # [M, N]\n</code></pre>"},{"location":"blog/2021-10-13-mmt4d/#performance-results","title":"Performance Results","text":"<p>We benchmarked various float32 matmul problems of different sizes and the result showed that mmt4d is faster than the existing matmul implementation for bigger matrices as we can see the in the following chart:</p> <p></p> <p>The SIMD instruction being used here is the simplest kind, a <code>vector*scalar</code> multiplication, and the storage orders of the matrices allow the existing implementation to directly load the vectors from the source matrices without any rearrangement overhead. So this case is particularly friendly to the existing code, which is why the mmt4d code is only faster for bigger matrices. To understand why mmt4d is faster in that case, we collected statistics of L1 cache misses:</p> <p></p> <p>This shows that in this case, the better cache-friendliness of mmt4d, thanks to its simple contiguous memory access pattern, accounts for its higher performance.</p> <p>As we proceed with increasingly sophisticated SIMD targets, starting with the dot-product instructions found in current mobile devices for the int8 case and going to become generalized to all data types all the way to float32 over the next few years with upcoming ARM SIMD instructions, the advantage of mmt4d will widen for all sizes, not just the larger ones.</p> <p>Part of why we feel confident about the eventual performance that our approach will achieve is that, as mentioned in the introduction, we are rebuilding within the compiler an existing library's schedule and kernel, and we have benchmark results about it.</p>"},{"location":"blog/2021-10-13-mmt4d/#conclusion","title":"Conclusion","text":"<p>We introduced a 4d tiled representation for 2d matrix-matrix multiplication with a decomposable algebric transformations that requires only reshape and transpose of input operands, we discussed and empirically showed how that solves major drawbacks in row-major linear matmul by providing a flexible way to match different ISA layout along with better cache locality achieving near peak performance.</p> <p>As was mentioned in the introduction, this work in under active development and the next immediate steps are to prove the rest of the hypothesis by:</p> <ul> <li> <p>Handling dynamic sizes and padding to the next multiple of the target tile     size.</p> </li> <li> <p>Implementing the integer case (<code>int32 += int8 * int8</code>).</p> </li> <li> <p>Implementing the dispatch to different SIMD ISA variants at runtime.</p> </li> <li> <p>Implementing cache-friendly traversal for larger matmuls and multi-threading     by interfacing with IREE's runtime dispatch.</p> </li> <li> <p>Improving the generated code by fusing the 4d tiled layout with the     producers and consumers of the <code>linalg.mmt4d</code>.</p> </li> </ul>"},{"location":"blog/2021-10-15-cuda-backend/","title":"CUDA backend","text":"<p>Friday, October 15, 2021  By Thomas Raoux</p>"},{"location":"blog/2021-10-15-cuda-backend/#cuda-backend-in-iree","title":"CUDA Backend in IREE","text":"<p>IREE is being designed with re-targetability as a core goal: it should be possible to use IREE to target a broad spectrum of power regimes, from embedded systems to distributed clusters; and it should be possible to extend IREE to target new back-ends without having to reinvent the wheel each time.</p> <p>To explore this, we recently branched out from our initial focus on low-latency mobile deployments with a goal of using IREE to target data center workloads on Nvidia CUDA. This post describes how we quickly brought up a CUDA back-end for IREE and used it to train BERT, then shares some metrics and next steps.</p>"},{"location":"blog/2021-10-15-cuda-backend/#bring-up","title":"Bring up","text":""},{"location":"blog/2021-10-15-cuda-backend/#hal-support","title":"HAL support","text":"<p>IREE has a HAL API that abstract all the targets behind a common interface. The first step to supporting a CUDA target was to map the HAL API onto CUDA. We use the CUDA driver API to reduce dependencies and be closer to the hardware. The HAL API is based on other GPU APIs like Vulkan and Metal, so it was a natural fit for CUDA. The HAL API exposes memory allocations, basic fill and memset commands, kernel dispatch, and general command buffer handling. The original implementation uses the CUDA graph API as a graph maps naturally to command buffers. There is also an implementation using CUDA streams for comparison.</p> <p>HAL exposes an API that can be tested independently, even if we are not able to create CUDA kernels yet we can test a large portion of the CUDA driver using CTS tests. Those can be run to make sure a system has the required CUDA support.</p> <p></p>"},{"location":"blog/2021-10-15-cuda-backend/#compiler-support","title":"Compiler support","text":"<p>CUDA has an open source backend in LLVM generating PTX that we are leveraging. Therefore IREE can create NVVM (CUDA LLVM variant) and use LLVM's backend to generate PTX. The CUDA driver will do the \"last mile compilation\" at runtime to convert PTX into the GPU's native ISA.</p> <p>IREE compiler pipeline starts from linalg with tensor operands. A large part of the compiler is independent of the target.</p> <p>The linalg on tensor representation of the graph is broken up into dispatch regions that are processed by NVVM Codegen. A simple implementation of the compiler is to run bufferization and convert linalg to standard followed by conversion to NVVM/LLVM. Most of those transformation can re-use upstream MLIR transformations and share it with any other backend targeting LLVM IR. Leveraging MLIR conversion to LLVM will allow us to quickly go from a simple \"hello world\" to supporting full models.</p> <p>IREE code generation is based on MLIR infrastructure so each step can easily be tested independently using the MLIR lit framework.</p>"},{"location":"blog/2021-10-15-cuda-backend/#flatbuffer-definition","title":"FlatBuffer definition","text":"<p>Kernels are encoded in a FlatBuffer containing the PTX code as well as the workgroup size to use for the dispatch. This allows serialization of the kernels in the IR, it is then de-serialized by the HAL layer.</p> <pre><code>table CUDAExecutableDef {\n  // A map of entry point ordinals to string names as used in the shader\n  // library.\n  entry_points:[string];\n\n  // Block sizes for each entry point.\n  block_sizes:[CUDABlockSizeDef];\n\n  // PTX string of the module.\n  ptx_image:string;\n}\n</code></pre>"},{"location":"blog/2021-10-15-cuda-backend/#hello-world","title":"Hello world","text":"<p>Together those 3 steps are enough to provide most of the functionality and we can now successfully compile full models.</p> <p></p> <p>The steps to reproduce running a simple op end to end through CUDA backend are described here.</p>"},{"location":"blog/2021-10-15-cuda-backend/#performance","title":"Performance","text":"<p>Now that we have enabled functionality we need to look at the performance. Once again we can leverage existing MLIR transformations to speed up the developement work.</p>"},{"location":"blog/2021-10-15-cuda-backend/#tiling-and-distribution","title":"Tiling and distribution","text":"<p>The first obvious step to get efficient code on CUDA is to make sure we distribute the work on enough blocks and threads to fill up the GPU. At the time of bring up not all ops were being tiled and distributed in the common IREE layer. During dispatch region creation we apply tile and fuse which will distribute the work into a set of workgroups that are mapped to CUDA blocks.</p> <p>At the beginning of the code generation we look at the dispatch region and decide on the tile size for a workgroup. For CUDA we also decide the number of threads per block. We will then have a pass tiling the ops in the dispatch region a second time to distribute the work onto threads within the block.</p> <p>At this stage the IR looks like the following: <pre><code>    %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -&gt; index\n    %9 = affine.apply affine_map&lt;()[s0] -&gt; (s0 * 4)&gt;()[%8]\n    %10 = memref.subview %in0[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %11 = memref.subview %in1[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %12 = memref.subview %out[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    linalg.generic {\n        indexing_maps = [affine_map&lt;(d0) -&gt; (d0)&gt;,\n                         affine_map&lt;(d0) -&gt; (d0)&gt;,\n                         affine_map&lt;(d0) -&gt; (d0)&gt;],\n        iterator_types = [\"parallel\"]}\n      ins(%10, %11 :\n          memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;,\n          memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;)\n      outs(%12 : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;) {\n    ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):  // no predecessors\n      %13 = addf %arg1, %arg2 : f32\n      linalg.yield %13 : f32\n    }\n</code></pre></p>"},{"location":"blog/2021-10-15-cuda-backend/#vectorization","title":"Vectorization","text":"<p>Even though GPUs execute most operations as scalar, memory operations are optimized to access 128 bits of data per thread. Therefore it is critical to vectorize load/store operations. After tiling to a size we vectorize the IR to get vector read/write mapping to load4/store4. This significantly improves the memory access pattern of the code generated.</p> <p>This convert the previous IR to: <pre><code>    %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -&gt; index\n    %9 = affine.apply affine_map&lt;()[s0] -&gt; (s0 * 4)&gt;()[%8]\n    %10 = memref.subview %in0[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %11 = memref.subview %in1[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %12 = memref.subview %out[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %13 = vector.transfer_read %10[%c0], %cst {in_bounds = [true]} : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;, vector&lt;4xf32&gt;\n    %14 = vector.transfer_read %11[%c0], %cst {in_bounds = [true]} : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;, vector&lt;4xf32&gt;\n    %15 = addf %13, %14 : vector&lt;4xf32&gt;\n    vector.transfer_write %15, %12[%c0] {in_bounds = [true]} : vector&lt;4xf32&gt;, memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n</code></pre></p>"},{"location":"blog/2021-10-15-cuda-backend/#shared-memory-optimization","title":"Shared memory optimization","text":"<p>Nvidia GPUs have a fast shared memory that needs to be leveraged to optimize cases where we may be memory bound and have the potential to re-use memory reads.</p> <p>For operations like GEMM using shared memory gives us a significant speed up. We leverage memory promotion, vector distribution and software pipelining transformations from MLIR to generate efficient copies from global to shared memory that can be interleaved with the compute work.</p>"},{"location":"blog/2021-10-15-cuda-backend/#optimization-pipeline","title":"Optimization pipeline","text":"<p>Those different transformations compose to this flow:</p> <p></p> <p>The full dump step by step of a linalg.matmul operation can be found here.</p>"},{"location":"blog/2021-10-15-cuda-backend/#results-and-next-steps","title":"Results and next steps","text":""},{"location":"blog/2021-10-15-cuda-backend/#gemm","title":"GEMM","text":"<p>We compare the performance of a single GEMM operation to highly optimized library cuBLAS using mmperf framework.</p> <p></p> <p>The graph can be re-produced based on instructions on mmperf</p>"},{"location":"blog/2021-10-15-cuda-backend/#future-work","title":"Future work","text":"<p>Nod.ai has contributed an experimental HAL module for ROCM that allows us to re-use the compiler parts to support ROCM, more support is going to be added in the future.</p> <p>Several performance improvements are still under progress, including optimizing the runtime allocator to reduce the host-side overhead and tuning tile sizes based profiling.</p> <p>Several models are running and we will publish more detailed benchmark results in the near future.</p>"},{"location":"building-from-source/","title":"Building IREE from source","text":"<p>While IREE does offer binary distributions for its compiler tools and Python bindings, building from source is still useful when using IREE's runtime or when making changes to the compiler or import tools themselves.</p>"},{"location":"building-from-source/#reference-pages","title":"Reference pages","text":"<ul> <li>Getting started</li> <li>Bindings and importers</li> <li>Android cross-compilation</li> <li>iOS cross-compilation</li> <li>RISC-V cross-compilation</li> </ul>"},{"location":"building-from-source/android/","title":"Android cross-compilation","text":"<p>Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture):</p> <ul> <li>IREE's compiler is built on the host and is used there to generate modules   for the target</li> <li>IREE's runtime is built on the host for the target. The runtime is then   either pushed to the target to run natively or is bundled into an Android   APK</li> </ul>"},{"location":"building-from-source/android/#prerequisites","title":"Prerequisites","text":""},{"location":"building-from-source/android/#host-environment-setup","title":"Host environment setup","text":"<p>You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.</p>"},{"location":"building-from-source/android/#install-android-ndk-and-adb","title":"Install Android NDK and ADB","text":"<p>The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here, or, if you have installed Android Studio, you can follow this guide instead.</p> <p>Note</p> <p>Make sure the <code>ANDROID_NDK</code> environment variable is set after installing the NDK.</p> <p>ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide.</p>"},{"location":"building-from-source/android/#configure-and-build","title":"Configure and build","text":""},{"location":"building-from-source/android/#host-configuration","title":"Host configuration","text":"<p>Build and install on your host machine:</p> <pre><code>cmake -GNinja -B ../iree-build/ \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n.\ncmake --build ../iree-build/ --target install\n</code></pre>"},{"location":"building-from-source/android/#target-configuration","title":"Target configuration","text":"<p>Build the runtime using the Android NDK toolchain:</p> Linux and MacOSWindows <pre><code>cmake -GNinja -B ../iree-build-android/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"${ANDROID_NDK?}/build/cmake/android.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=\"$PWD/../iree-build/install/bin\" \\\n-DANDROID_ABI=\"arm64-v8a\" \\\n-DANDROID_PLATFORM=\"android-29\" \\\n-DIREE_BUILD_COMPILER=OFF \\\n.\ncmake --build ../iree-build-android/\n</code></pre> <pre><code>cmake -GNinja -B ../iree-build-android/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=\"%CD%/../iree-build/install/bin\" \\\n-DANDROID_ABI=\"arm64-v8a\" \\\n-DANDROID_PLATFORM=\"android-29\" \\\n-DIREE_BUILD_COMPILER=OFF \\\n.\ncmake --build ../iree-build-android/\n</code></pre> <p>Note</p> <p>See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android.</p> <p>The specific <code>ANDROID_ABI</code> and <code>ANDROID_PLATFORM</code> used should match your target device.</p>"},{"location":"building-from-source/android/#running-android-tests","title":"Running Android tests","text":"<p>Make sure you enable developer options and USB debugging on your Android device and can see your it when you run <code>adb devices</code>, then run all built tests through CTest:</p> <pre><code>ctest --test-dir ../iree-build-android/ --output-on-failure\n</code></pre> <p>This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.</p>"},{"location":"building-from-source/android/#running-tools-directly","title":"Running tools directly","text":"<p>Invoke the host compiler tools to produce a bytecode module FlatBuffer:</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>Push the Android runtime tools to the device, along with any FlatBuffer files:</p> <pre><code>adb push ../iree-build-android/tools/iree-run-module /data/local/tmp/\nadb shell chmod +x /data/local/tmp/iree-run-module\nadb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/\n</code></pre> <p>Run the tool:</p> <pre><code>adb shell /data/local/tmp/iree-run-module --device=local-task \\\n--module=/data/local/tmp/simple_abs_vmvx.vmfb \\\n--function=abs \\\n--input=\"f32=-5\"\n</code></pre>"},{"location":"building-from-source/getting-started/","title":"Getting started","text":""},{"location":"building-from-source/getting-started/#prerequisites","title":"Prerequisites","text":"<p>You will need to install CMake, the Ninja CMake generator, and the clang or MSVC C/C++ compilers. The tests also requires Python3 and the python package requests to run.</p> Note <p>You are welcome to try different CMake generators and compilers, but IREE devs and CIs exclusively use these and other configurations are \"best effort\". Additionally, compilation on macOS is \"best effort\" as well, though we generally expect it to work due to its similarity with Linux. Patches to improve support for these are always welcome.</p> LinuxmacOSWindows <ol> <li> <p>Install a compiler/linker (typically \"clang\" and \"lld\" package)</p> </li> <li> <p>Install CMake (typically \"cmake\" package)</p> </li> <li> <p>Install Ninja (typically \"ninja-build\"    package)</p> </li> </ol> <p>On a relatively recent Debian/Ubuntu:</p> <pre><code>sudo apt install cmake ninja-build clang lld\n</code></pre> <ol> <li> <p>Install CMake (typically \"cmake\" package)</p> </li> <li> <p>Install Ninja (typically \"ninja-build\"    package)</p> </li> </ol> <p>If using Homebrew:</p> <pre><code>brew install cmake ninja\n</code></pre> <ol> <li> <p>Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the    official downloads page</p> </li> <li> <p>Install CMake from the    official downloads page</p> </li> <li> <p>Install Ninja either from the    official site</p> </li> </ol> <p>Note</p> <p>You will need to initialize MSVC by running <code>vcvarsall.bat</code> to use it from the command line. See the official documentation for details.</p>"},{"location":"building-from-source/getting-started/#clone-and-build","title":"Clone and build","text":"<p>Use Git to clone the IREE repository and initialize its submodules:</p> <pre><code>git clone https://github.com/iree-org/iree.git\ncd iree\ngit submodule update --init\n</code></pre> <p>Configure then build all targets using CMake:</p> <p>Configure CMake:</p> Linux and MacOSWindows <pre><code># Recommended for simple development using clang and lld:\ncmake -GNinja -B ../iree-build/ -S . \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_ENABLE_ASSERTIONS=ON \\\n-DCMAKE_C_COMPILER=clang \\\n-DCMAKE_CXX_COMPILER=clang++ \\\n-DIREE_ENABLE_LLD=ON\n\n# Alternately, with system compiler and your choice of CMake generator:\n# cmake -B ../iree-build/ -S .\n\n# Additional quality of life CMake flags:\n# Enable ccache:\n# See https://github.com/iree-org/iree/blob/main/docs/developers/developing_iree/ccache.md\n#   -DCMAKE_C_COMPILER_LAUNCHER=ccache\n#   -DCMAKE_CXX_COMPILER_LAUNCHER=ccache\n</code></pre> <pre><code>cmake -GNinja -B ../iree-build/ -S . \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_ENABLE_ASSERTIONS=ON\n</code></pre> <p>Build:</p> <pre><code>cmake --build ../iree-build/\n</code></pre> Tip <p>We recommend using the <code>RelWithDebInfo</code> build type by default for a good balance of debugging information and performance. The <code>Debug</code>, <code>Release</code>, and <code>MinSizeRel</code> build types are useful in more specific scenarios. In particular, note that several useful LLVM debugging features are only available in <code>Debug</code> builds. See the official CMake documentation for general details.</p>"},{"location":"building-from-source/getting-started/#whats-next","title":"What's next?","text":""},{"location":"building-from-source/getting-started/#running-tests","title":"Running tests","text":"<p>Build test dependencies:</p> <pre><code>cmake --build ../iree-build --target iree-test-deps\n</code></pre> <p>Run all built tests through CTest:</p> <pre><code>ctest --test-dir ../iree-build/ --output-on-failure\n</code></pre>"},{"location":"building-from-source/getting-started/#take-a-look-around","title":"Take a look around","text":"<p>Check out the contents of the 'tools' build directory:</p> <pre><code>ls ../iree-build/tools/\n../iree-build/tools/iree-compile --help\n</code></pre>"},{"location":"building-from-source/ios/","title":"iOS cross-compilation","text":"<p>Cross-compilation for iOS consists of the two steps below.</p> <ul> <li>On the macOS host, build the IREE compiler.  We can run it to create   IREE modules.</li> <li>Build the IREE runtime on the macOS host for iOS devices and the   simulator.  We can then run the IREE module on the simulator.</li> </ul>"},{"location":"building-from-source/ios/#prerequisites","title":"Prerequisites","text":""},{"location":"building-from-source/ios/#install-xcode-and-ios-sdk","title":"Install Xcode and iOS SDK","text":"<p>For cross-compilation, you need Xcode. It comes with the SDKs for iOS devices and the simulator, as well as the <code>simctl</code> tool for controlling the simulator from the command line.</p>"},{"location":"building-from-source/ios/#host-environment-setup","title":"Host environment setup","text":"<p>On your host platform, you should already be able to build IREE from source.  Please make sure you've gone through the steps in getting started.</p>"},{"location":"building-from-source/ios/#configure-and-build","title":"Configure and Build","text":""},{"location":"building-from-source/ios/#build-the-iree-compiler-for-the-host","title":"Build the IREE Compiler for the Host","text":"<p>Build and install on your macOS host:</p> <pre><code>cmake -S . -B ../iree-build/ -GNinja \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install\n\ncmake --build ../iree-build/ --target install\n</code></pre>"},{"location":"building-from-source/ios/#cross-compile-the-iree-runtime-for-ios","title":"Cross-compile the IREE Runtime for iOS","text":"<p>Build the runtime for the iOS Simulator.</p> <pre><code>cmake -S . -B ../build-ios-sim -GNinja \\\n-DCMAKE_SYSTEM_NAME=iOS \\\n-DCMAKE_OSX_SYSROOT=$(xcodebuild -version -sdk iphonesimulator Path) \\\n-DCMAKE_OSX_ARCHITECTURES=arm64 \\\n-DCMAKE_SYSTEM_PROCESSOR=arm64 \\\n-DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 \\\n-DCMAKE_IOS_INSTALL_COMBINED=YES \\\n-DIREE_HOST_BIN_DIR=\"$PWD/../iree-build/install/bin\" \\\n-DCMAKE_INSTALL_PREFIX=../build-ios-sim/install \\\n-DIREE_BUILD_COMPILER=OFF\n\ncmake --build ../build-ios-sim --config Release --target install\n</code></pre> <p>Or, we can build the runtime for iOS devices it by changing the value of the <code>-DCMAKE OSX SYSROOT</code> option to:</p> <pre><code>  -DCMAKE_OSX_SYSROOT=$(xcodebuild -version -sdk iphoneos Path)\n</code></pre>"},{"location":"building-from-source/ios/#running-iree-modules-on-the-ios-simulator","title":"Running IREE Modules on the iOS Simulator","text":"<p>Run the IREE compiler on the host to generate a module.</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>We could test the generated module by running the macOS version of <code>iree-run-module</code> on the host.</p> <pre><code>../iree-build/install/bin/iree-run-module \\\n--module=/tmp/simple_abs_vmvx.vmfb \\\n--device=local-task \\\n--function=abs \\\n--input=\"f32=-5\"\n</code></pre> <p>To run it on the iOS simulator, we need to copy the vmfb file into the <code>iree-run-module</code> iOS app bundle.</p> <pre><code>cp /tmp/simple_abs_vmvx.vmfb \\\n   ../build-ios-sim/install/bin/iree-run-module.app/\n</code></pre> <p>Open the iOS Simulator Manager on the host.</p> <pre><code>open -a Simulator\n</code></pre> <p>After creating and booting a simulator in this app, you can list it from the command-line.</p> <pre><code>xcrun simctl list devices | grep Booted\n</code></pre> <p>This is what should come out of the command:</p> <pre><code>    iPhone 14 Pro (12341234-ABCD-ABCD-ABCD-123412341234) (Booted)\n</code></pre> <p>where <code>iPhone 14 Pro</code> is the device being simulated and <code>12341234-ABCD-ABCD-ABCD-123412341234</code> is the simulator's unique device ID (UDID).</p> <p>Install the app <code>iree-run-module</code> on the simulator, given its UDID.</p> <pre><code>xcrun simctl install &lt;UDID&gt; ../build-ios-sim/install/bin/iree-run-module.app\n</code></pre> <p>Check the path to the installed bundle, where the <code>simple_abs_vmvx.vmfb</code> module should be found.</p> <pre><code>ls $(xcrun simctl get_app_container &lt;UDID&gt; dev.iree.iree-run-module)\n</code></pre> <p>The string <code>dev.iree.iree-run-module</code> is the bundle identifier of the iOS app.  The CMake building process generates it and saves it in the property list (plist) file <code>../build-ios-sim/install/bin/iree-run-module.app/Info.plist</code>.</p> <p>Launch the <code>iree-run-module</code> app on the simulator to run the IREE module <code>simple_abs_vmvx.vmfb</code>.</p> <pre><code>xcrun simctl launch --console \\\n&lt;UDID&gt; \\\ndev.iree.runmodule \\\n--device=local-task \\\n--function=abs \\\n--input=\"f32=-5\" \\\n--module=$(xcrun simctl get_app_container &lt;UDID&gt; dev.iree.iree-run-module)/simple_abs_vmvx.vmfb\n</code></pre>"},{"location":"building-from-source/python-bindings-and-importers/","title":"Python bindings and importers","text":"<p>Attention</p> <p>These components are more complicated to build from source than the rest of the project. If your usage does not require making source changes, prefer to install from the official binary distributions instead.</p> <p>This page covers how to build IREE's Python-based bindings and import tools from source. These components are built using CMake as well as other dependencies and each section extends the basic build steps in the getting started page.</p>"},{"location":"building-from-source/python-bindings-and-importers/#building-python-bindings","title":"Building Python bindings","text":"<p>This section describes how to build and interactively use built-from-source Python bindings for the following packages:</p> Python Import Description <code>import iree.compiler</code> IREE's generic compiler tools and helpers <code>import iree.runtime</code> IREE's runtime, including CPU and GPU backends <p>Also see instructions for installing pre-built binaries.</p> <p>Pre-requisites:</p> <ul> <li>A relatively recent Python3 installation &gt;=3.7 (we aim to support   non-eol Python versions).</li> </ul> <p>CMake Variables:</p> <ul> <li> <p><code>IREE_BUILD_PYTHON_BINDINGS</code> : <code>BOOL</code></p> <p>Enables building of Python bindings under <code>runtime/bindings/python</code> in the repository. Defaults to <code>OFF</code>.</p> </li> <li> <p><code>Python3_EXECUTABLE</code> : <code>PATH</code></p> <p>Full path to the Python3 executable to build against. If not specified, CMake will auto-detect this, which often yields incorrect results on systems with multiple Python versions. Explicitly setting this is recommended. Note that mixed case of the option.</p> </li> </ul>"},{"location":"building-from-source/python-bindings-and-importers/#environment-setup","title":"Environment setup","text":"<p>We recommend using virtual environments to manage python packages, such as through <code>venv</code>, which may need to be installed via your system package manager (about, tutorial):</p> Linux and MacOSWindows <pre><code># Make sure your 'python' is what you expect. Note that on multi-python\n# systems, this may have a version suffix, and on many Linuxes and MacOS where\n# python2 and python3 co-exist, you may also want to use `python3`.\nwhich python\npython --version\n\n# Create a persistent virtual environment (first time only).\npython -m venv iree.venv\n\n# Activate the virtual environment (per shell).\n# Now the `python` command will resolve to your virtual environment\n# (even on systems where you typically use `python3`).\nsource iree.venv/bin/activate\n\n# Upgrade PIP. On Linux, many packages cannot be installed for older\n# PIP versions. See: https://github.com/pypa/manylinux\npython -m pip install --upgrade pip\n\n# Install IREE build pre-requisites.\npython -m pip install -r ./runtime/bindings/python/iree/runtime/build_requirements.txt\n</code></pre> <pre><code># Make sure your 'python' is what you expect.\n# Also consider using the Python launcher 'py' instead of 'python':\n# https://docs.python.org/3/using/windows.html#python-launcher-for-windows\nwhich python\npython --version\npy --list-paths\n\n# Create a persistent virtual environment (first time only).\npython -m venv .venv\n\n# Activate the virtual environment (per shell).\n# Now the `python` command will resolve to your virtual environment\n# (even on systems where you typically use `python3`).\n.venv\\Scripts\\activate.bat\n\n# Upgrade PIP.\npython -m pip install --upgrade pip\n\n# Install IREE build pre-requisites.\npython -m pip install -r runtime\\bindings\\python\\iree\\runtime\\build_requirements.txt\n</code></pre> <p>When you are done with the venv, you can close it by closing your shell or running <code>deactivate</code>.</p>"},{"location":"building-from-source/python-bindings-and-importers/#building-with-cmake","title":"Building with CMake","text":"<p>From the <code>iree-build</code> directory:</p> Linux and MacOSWindows <pre><code>cmake \\\n-GNinja \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_BUILD_PYTHON_BINDINGS=ON \\\n-DPython3_EXECUTABLE=\"$(which python)\" \\\n.\ncmake --build .\n\n# Add the bindings/python paths to PYTHONPATH and use the API.\nsource .env &amp;&amp; export PYTHONPATH\npython -c \"import iree.compiler\"\npython -c \"import iree.runtime\"\n</code></pre> <pre><code>cmake -GNinja -DCMAKE_BUILD_TYPE=RelWithDebInfo -DIREE_BUILD_PYTHON_BINDINGS=ON .\ncmake --build .\n\n# Add the bindings/python paths to PYTHONPATH and use the API.\n.env.bat\npython -c \"import iree.compiler\"\npython -c \"import iree.runtime\"\n</code></pre> <p>Tests can now be run individually via python or via ctest.</p>"},{"location":"building-from-source/python-bindings-and-importers/#building-tensorflow-frontend-bindings","title":"Building TensorFlow frontend bindings","text":"<p>This section describes how to build compiler tools and Python bindings for importing models from various TensorFlow-ecosystem frontends, including TensorFlow, XLA (used for JAX), and TFLite. It extends the instructions in Building Python Bindings above with additional steps that are TensorFlow specific. There are various ways to achieve these ends, but this section describes the canonical method that the core developers recommend. Upon completing these steps, you will have access to additional Python packages:</p> Python Import Description <code>import iree.compiler.tools.tf</code> Tools for importing from TensorFlow <code>import iree.compiler.tools.tflite</code> Tools for importing from TensorFlow Lite <code>import iree.compiler.tools.xla</code> Tools for importing from XLA <p>These tools packages are needed in order for the frontend specific, high-level APIs under <code>import iree.compiler.tf</code>, <code>import iree.compiler.tflite</code>, <code>import iree.compiler.xla</code>, and <code>import iree.jax</code> to be fully functional.</p> <p>Warning</p> <p>This section is under construction. Refer to the source documentation for the latest building from source instructions.</p> Note <p>Due to the difficulties using Bazel and compiling TensorFlow, only compilation on Linux with clang is supported. Other OS's and compilers are \"best effort\" with patches to improve support welcome.</p>"},{"location":"building-from-source/riscv/","title":"RISC-V cross-compilation","text":"<p>Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system):</p> <ul> <li>IREE's compiler is built on the host and is used there to generate modules   for the target</li> <li>IREE's runtime is built on the host for the target. The runtime is then   pushed to the target to run natively.</li> </ul>"},{"location":"building-from-source/riscv/#prerequisites","title":"Prerequisites","text":""},{"location":"building-from-source/riscv/#host-environment-setup","title":"Host environment setup","text":"<p>You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.</p>"},{"location":"building-from-source/riscv/#install-risc-v-cross-compile-toolchain-and-emulator","title":"Install RISC-V cross-compile toolchain and emulator","text":"<p>You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator.</p> <p>See instructions in the following links</p> <ul> <li>Clang getting started</li> <li>RISC-V GNU toolchain</li> <li>QEMU</li> <li>RISC-V Linux QEMU</li> </ul> <p>Note</p> <p>The <code>RISCV_TOOLCHAIN_ROOT</code> environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library.</p>"},{"location":"building-from-source/riscv/#install-prebuilt-risc-v-tools-risc-v-64-bit-linux-toolchain","title":"Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain)","text":"<p>Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory:</p> <pre><code>./build_tools/riscv/riscv_bootstrap.sh\n</code></pre>"},{"location":"building-from-source/riscv/#support-vector-extension","title":"Support vector extension","text":"<p>For RISC-V vector extensions support, see additional instructions</p>"},{"location":"building-from-source/riscv/#configure-and-build","title":"Configure and build","text":""},{"location":"building-from-source/riscv/#host-configuration","title":"Host configuration","text":"<p>Build and install on your host machine:</p> <pre><code>cmake -GNinja -B ../iree-build/ \\\n-DCMAKE_C_COMPILER=clang \\\n-DCMAKE_CXX_COMPILER=clang++ \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n.\ncmake --build ../iree-build/ --target install\n</code></pre>"},{"location":"building-from-source/riscv/#target-configuration","title":"Target configuration","text":"<p>The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration.</p>"},{"location":"building-from-source/riscv/#risc-v-64-bit-linux-target","title":"RISC-V 64-bit Linux target","text":"<pre><code>cmake -GNinja -B ../iree-build-riscv/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"./build_tools/cmake/riscv.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=$(realpath ../iree-build/install/bin) \\\n-DRISCV_CPU=linux-riscv_64 \\\n-DIREE_BUILD_COMPILER=OFF \\\n-DRISCV_TOOLCHAIN_ROOT=${RISCV_TOOLCHAIN_ROOT} \\\n-DIREE_ENABLE_CPUINFO=OFF \\\n.\ncmake --build ../iree-build-riscv/\n</code></pre>"},{"location":"building-from-source/riscv/#running-iree-bytecode-modules-on-the-risc-v-system","title":"Running IREE bytecode modules on the RISC-V system","text":"<p>Note</p> <p>The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine.</p> <p>Set the path to qemu-riscv64 Linux emulator binary in the <code>QEMU_BIN</code> environment variable. If it is installed with <code>riscv_bootstrap.sh</code>, the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64.</p> <pre><code>export QEMU_BIN=&lt;path to qemu-riscv64 binary&gt;\n</code></pre> <p>Invoke the host compiler tools to produce a bytecode module FlatBuffer:</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>Run the RISC-V emulation:</p> <pre><code>${QEMU_BIN} \\\n-cpu rv64 \\\n-L ${RISCV_TOOLCHAIN_ROOT}/sysroot/ \\\n../iree-build-riscv/tools/iree-run-module \\\n--device=local-task \\\n--module=/tmp/simple_abs_vmvx.vmfb \\\n--function=abs \\\n--input=f32=-5\n</code></pre>"},{"location":"building-from-source/riscv/#optional-configuration","title":"Optional configuration","text":"<p>RISC-V Vector extensions allows SIMD  code to run more efficiently. To enable the vector extension for the compiler  toolchain and the emulator, build the tools from the following sources:</p> <ul> <li>RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch).<ul> <li>Currently, the LLVM compiler is built on GNU toolchain, including libgcc,   GNU linker, and C libraries. You need to build GNU toolchain first.</li> <li>Clone GNU toolchain from:   https://github.com/riscv/riscv-gnu-toolchain   (master branch). Switch the \"riscv-binutils\" submodule to   <code>git://sourceware.org/git/binutils-gdb.git</code> (master branch) manually.</li> </ul> </li> <li>RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh.</li> </ul> <p>The SIMD code can be generated following the IREE CPU flow with the additional command-line flags</p> <pre><code>tools/iree-compile \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-llvm-target-triple=riscv64 \\\n--iree-llvm-target-cpu=generic-rv64 \\\n--iree-llvm-target-abi=lp64d \\\n--iree-llvm-target-cpu-features=\"+m,+a,+f,+d,+zvl512b,+v\" \\\n--riscv-v-fixed-length-vector-lmul-max=8 \\\niree_input.mlir -o mobilenet_cpu.vmfb\n</code></pre> <p>Then run on the RISC-V QEMU:</p> <pre><code>${QEMU_BIN} \\\n-cpu rv64,x-v=true,x-k=true,vlen=512,elen=64,vext_spec=v1.0 \\\n-L ${RISCV_TOOLCHAIN_ROOT}/sysroot/ \\\n../iree-build-riscv/tools/iree-run-module \\\n--device=local-task \\\n--module=mobilenet_cpu.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre>"},{"location":"community/","title":"Community projects","text":"<p>Projects built by community members:</p> <ul> <li> <p>The SHARK project from     nod.ai uses a forked version of IREE     (SHARK-Runtime), offering     highly tuned performance on a large corpus of machine learning programs.</p> </li> <li> <p>The IREE Bare-Metal Arm Sample     shows how to build IREE with the     Arm GNU Toolchain     for bare-metal Arm targets using the open-source firmware libraries     CMSIS and     libopencm3.</p> </li> <li> <p>The IREE C++ Template     shows one way to integrate IREE's runtime into a project with CMake.</p> </li> </ul> <p>Official repositories:</p> <ul> <li> <p>iree-jax is home to     IREE's support for JAX programs.</p> </li> <li> <p>iree-torch contains     IREE's PyTorch frontend, leveraging the     torch-mlir project.</p> </li> <li> <p>iree-samples     includes various samples and prototypes built with IREE.</p> </li> <li> <p>iree-llvm-sandbox     contains experimental work by the IREE team closely related to LLVM and     MLIR, usually with the aim of contributing back to those upstream projects.</p> </li> </ul>"},{"location":"deployment-configurations/","title":"Deployment configurations","text":"<p>IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE to load programs on demand and to take advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators.</p>"},{"location":"deployment-configurations/#stable-configurations","title":"Stable configurations","text":"<ul> <li>CPU for general purpose CPU deployment</li> <li>CPU - Bare-Metal with minimal platform dependencies</li> <li>GPU - Vulkan</li> <li>GPU - CUDA/ROCm</li> </ul> <p>These are just the most stable configurations IREE supports. Feel free to reach out on any of IREE's communication channels if you have questions about a specific platform, hardware accelerator, or set of system features.</p>"},{"location":"deployment-configurations/#compiler-target-backends","title":"Compiler target backends","text":"<p>Compiler target backends are used to generate executable code for hardware APIs and device architectures. Compiler targets may implement special optimizations or generate distinct code for certain device/architecture/performance profiles.</p> <p>When compiling programs, a list of target backends must be specified via</p> <ul> <li><code>--iree-hal-target-backends=</code> (command-line)</li> <li><code>target_backends=[...]</code> (Python)</li> </ul> Target backend Description Compatible HAL devices <code>llvm-cpu</code> Code generation for CPU-like devices supported by LLVM <code>local-sync</code>, <code>local-task</code> <code>vmvx</code> Portable interpreter powered by a microkernel library <code>local-sync</code>, <code>local-task</code> <code>vulkan-spirv</code> Portable GPU support via SPIR-V for Vulkan <code>vulkan</code> <code>cuda</code> NVIDIA GPU support via PTX for CUDA <code>cuda</code> <code>rocm</code> Experimental  AMD GPU support via HSACO for ROCm <code>rocm</code> <code>webgpu-wgsl</code> Experimental  GPU support on the Web via WGSL for WebGPU <code>webgpu</code> <code>metal-spirv</code> Stale - see Issue#4370  GPU support on Apple platforms via MSL for Metal <code>metal</code> <p>Tip</p> <p>The list of available compiler target backends can be queried with</p> <ul> <li><code>iree-compile --iree-hal-list-target-backends</code> (command-line)</li> <li><code>iree.compiler.query_available_targets()</code> (Python)</li> </ul>"},{"location":"deployment-configurations/#runtime-hal-driversdevices","title":"Runtime HAL drivers/devices","text":"<p>Runtime HAL devices call into hardware APIs to load and run executable code. Devices may use multithreading or other system resources, depending on their focus and the build configuration.</p> HAL device Description <code>local-sync</code> Synchronous local CPU device with inline execution <code>local-task</code> Multithreaded local CPU device using a 'task' executor <code>vulkan</code> Portable GPU execution using the Vulkan API <code>cuda</code> NVIDIA GPU execution using CUDA <code>rocm</code> Experimental  AMD GPU execution using ROCm <code>webgpu</code> Experimental  GPU execution on the web using WebGPU <code>metal</code> Stale - see Issue#4370  GPU execution on Apple platforms using Metal <p>Tip</p> <p>The list of available runtime HAL devices can be queried with</p> <ul> <li><code>iree-run-module --list_devices</code> (command-line)</li> <li><code>iree.runtime.query_available_drivers()</code> (Python)</li> </ul> <p>Additional HAL drivers can also be defined external to the core project via <code>IREE_EXTERNAL_HAL_DRIVERS</code>.</p>"},{"location":"deployment-configurations/bare-metal/","title":"Run on a Bare-Metal Platform","text":"<p>IREE supports CPU model execution on bare-metal platforms. That is, platforms without operating system support, for which executables are built using machine-specific linker scripts and/or board support packages (BSPs).</p> <p>Bare-metal deployment typically uses IREE's LLVM compiler target much like the CPU configuration, but using a limited subset of IREE's CPU HAL driver code at runtime to load and execute compiled programs.</p>"},{"location":"deployment-configurations/bare-metal/#prerequisites","title":"Prerequisites","text":"<p>Out-of-tree bare-metal platform tools and source code for the system should be ready, such as</p> <ul> <li>Compilation toolchain</li> <li>Platform linker script</li> <li>Firmware libraries</li> </ul> <p>Please follow the instructions to retrieve the IREE compiler.</p>"},{"location":"deployment-configurations/bare-metal/#compile-the-model-for-bare-metal","title":"Compile the model for bare-metal","text":"<p>The model can be compiled with the following command (assuming the path to <code>iree-compile</code> is in your system's <code>PATH</code>):</p> <pre><code>iree-compile \\\n--iree-stream-partitioning-favor=min-peak-memory \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-llvm-target-triple=x86_64-pc-linux-elf \\\n--iree-llvm-debug-symbols=false \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_cpu.vmfb\n</code></pre> <p>In which</p> <ul> <li><code>-iree-stream-partitioning-favor=min-peak-memory</code>: Optimize for minimum peak     memory usage at the cost of concurrency - include when targeting     single-threaded execution to reduce memory consumption.</li> <li><code>iree-hal-target-backends=llvm-cpu</code>: Compile using the LLVM CPU target</li> <li><code>iree-llvm-target-triple</code>: Use the <code>&lt;arch&gt;-pc-linux-elf</code> LLVM target triple     so the artifact has a fixed ABI to be rendered by the     elf_module library</li> <li><code>iree-llvm-debug-symbols=false</code>: To reduce the artifact size</li> </ul> <p>See generate.sh for example command-line instructions of some common architectures</p> <p>You can replace the MLIR file with the other MLIR model files, following the instructions</p>"},{"location":"deployment-configurations/bare-metal/#compiling-the-bare-metal-model-for-static-library-support","title":"Compiling the bare-metal model for static-library support","text":"<p>See the static_library demo sample for an example and instructions on running a model with IREE's <code>static_library_loader</code>.</p> <p>By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run <code>iree-compile</code> as in the previous example and add the additional <code>-iree-llvm-static-library-output-path=</code> flag to specify the static library destination. This will produce a <code>.h\\.o</code> file to link directly into the target application.</p>"},{"location":"deployment-configurations/bare-metal/#build-bare-metal-runtime-from-the-source","title":"Build bare-metal runtime from the source","text":"<p>A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion.</p>"},{"location":"deployment-configurations/bare-metal/#set-cmake-options","title":"Set CMake options","text":"<ul> <li><code>set(IREE_BUILD_COMPILER OFF)</code>: Build IREE runtime only</li> <li><code>set(CMAKE_SYSTEM_NAME Generic)</code>: Tell CMake to skip targeting a specific operating system</li> <li><code>set(IREE_BINDINGS_TFLITE OFF)</code>: Disable the TFLite binding support</li> <li><code>set(IREE_ENABLE_THREADING OFF)</code>: Disable multi-thread library support</li> <li><code>set(IREE_HAL_DRIVER_DEFAULTS OFF)</code>: Disable HAL drivers by default, then enable the synchronous HAL drivers with <code>set(IREE_HAL_DRIVER_LOCAL_SYNC ON)</code></li> <li><code>set(IREE_HAL_EXECUTABLE_LOADER_DEFAULTS OFF)</code>: Disable HAL executable loaders by default, then enable the CPU codegen and VMVX loaders with <code>set(IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF ON)</code> and <code>set(IREE_HAL_EXECUTABLE_LOADER_VMVX_MODULE ON)</code></li> <li><code>set(IREE_BUILD_TESTS OFF)</code>: Disable tests until IREE supports running them on bare-metal platforms</li> <li><code>set(IREE_BUILD_SAMPLES ON)</code>: Build simple_embedding example</li> </ul> <p>Todo</p> <p>Clean the list up after #6353 is fixed.</p> <p>Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc.</p>"},{"location":"deployment-configurations/bare-metal/#define-iree-macros","title":"Define IREE macros","text":"<ul> <li><code>-DIREE_PLATFORM_GENERIC</code>: Let IREE to build the runtime library without targeting a specific platform.</li> <li><code>-DIREE_SYNCHRONIZATION_DISABLE_UNSAFE=1</code>: Disable thread synchronization support. Must only be used if there's a single thread.</li> <li><code>-DIREE_FILE_IO_ENABLE=0</code>: Disable file I/O.</li> <li><code>-DIREE_TIME_NOW_FN</code>: A function to return the system time. For the bare-metal system, it can be set as <code>-DIREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\"</code> as there's no asynchronous wait handling.</li> <li><code>-DIREE_WAIT_UNTIL_FN</code>: A function to wait until the given time in nanoseconds. Must match the signature <code>bool(uint64_t nanos)</code> and return false if the wait failed.</li> </ul> <p>Examples of how to setup the CMakeLists.txt and .cmake file:</p> <ul> <li>IREE RISC-V toolchain cmake</li> <li>IREE Bare-Metal Arm Sample</li> <li>IREE Bare-Metal RV32 Sample</li> </ul>"},{"location":"deployment-configurations/bare-metal/#bare-metal-execution-example","title":"Bare-metal execution example","text":"<p>See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.</p>"},{"location":"deployment-configurations/cpu/","title":"CPU Deployment","text":"<p>IREE supports efficient program execution on CPU devices by using LLVM to compile all dense computations in each program into highly optimized CPU native instruction streams, which are embedded in one of IREE's deployable formats.</p> <p>To compile a program for CPU execution, pick one of IREE's supported executable formats:</p> Executable Format Description embedded ELF portable, high performance dynamic library system library platform-specific dynamic library (.so, .dll, etc.) VMVX reference target <p>At runtime, CPU executables can be loaded using one of IREE's CPU HAL drivers:</p> <ul> <li><code>local-task</code>: asynchronous, multithreaded driver built on IREE's \"task\"    system</li> <li><code>local-sync</code>: synchronous, single-threaded driver that executes work inline</li> </ul> <p>Todo</p> <p>Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.</p>"},{"location":"deployment-configurations/cpu/#get-compiler-and-runtime","title":"Get compiler and runtime","text":""},{"location":"deployment-configurations/cpu/#get-compiler-for-cpu-native-instructions","title":"Get compiler for CPU native instructions","text":""},{"location":"deployment-configurations/cpu/#download-as-python-package","title":"Download as Python package","text":"<p>Python packages for various IREE functionalities are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the LLVM-based CPU compiler:</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable. <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre></p>"},{"location":"deployment-configurations/cpu/#build-compiler-from-source","title":"Build compiler from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation or iOS cross-compilation page if you are cross compiling for a mobile device. The LLVM (CPU) compiler backend is compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_TARGET_BACKEND_LLVM_CPU</code> CMake option is <code>ON</code> when configuring for the host.</p> <p>Tip</p> <p><code>iree-compile</code> is under <code>iree-build/tools/</code> directory. You may want to include this path in your system's <code>PATH</code> environment variable.</p>"},{"location":"deployment-configurations/cpu/#compile-and-run-the-model","title":"Compile and run the model","text":"<p>With the compiler and runtime for local CPU execution, we can now compile a model and run it.</p>"},{"location":"deployment-configurations/cpu/#compile-the-model","title":"Compile the model","text":"<p>The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by the IREE compiler first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then,</p>"},{"location":"deployment-configurations/cpu/#compile-using-the-command-line","title":"Compile using the command-line","text":"<p>Run the following command (passing <code>--iree-input-type=</code> as needed for your import tool):</p> <pre><code>iree-compile \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-input-type=mhlo \\\niree_input.mlir -o mobilenet_cpu.vmfb\n</code></pre> <p>where <code>iree_input.mlir</code> is the imported program.</p> <p>Tip</p> <p>The <code>--iree-llvm-target-triple=</code> flag tells the compiler to generate code for a specific type of CPU. You can see the list of supported targets with <code>iree-compile --iree-llvm-list-targets</code>, or omit the flag to let LLVM infer the triple from your host machine (e.g. <code>x86_64-linux-gnu</code>).</p>"},{"location":"deployment-configurations/cpu/#get-iree-runtime-with-local-cpu-hal-driver","title":"Get IREE runtime with local CPU HAL driver","text":"<p>You will need to get an IREE runtime that supports the local CPU HAL driver, along with the appropriate executable loaders for your application.</p>"},{"location":"deployment-configurations/cpu/#build-runtime-from-source","title":"Build runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The local CPU HAL drivers are compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_HAL_DRIVER_LOCAL_TASK</code> and <code>IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF</code> (or other executable loader) CMake options are <code>ON</code> when configuring for the target.</p>"},{"location":"deployment-configurations/cpu/#run-the-model","title":"Run the model","text":""},{"location":"deployment-configurations/cpu/#run-using-the-command-line","title":"Run using the command-line","text":"<p>In the build directory, run the following command:</p> <pre><code>tools/iree-run-module \\\n--device=local-task \\\n--module=mobilenet_cpu.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/","title":"CUDA and ROCm GPU HAL Driver","text":"<p>IREE can accelerate model execution on NVIDIA GPUs using CUDA and on AMD GPUs using ROCm. Due to the similarity of CUDA and ROCm APIs and infrastructure, the CUDA and ROCm backends share much of their implementation in IREE:</p> <ul> <li>The IREE compiler uses a similar GPU code generation pipeline for each, but generates PTX for CUDA and hsaco for ROCm</li> <li>The IREE runtime HAL driver for ROCm mirrors the one for CUDA, except for command buffers implementations - where CUDA has \"direct\", \"stream\", and \"graph\" command buffers, and ROCm has only \"direct\" command buffers</li> </ul>"},{"location":"deployment-configurations/gpu-cuda-rocm/#prerequisites","title":"Prerequisites","text":"<p>In order to use CUDA or ROCm to drive the GPU, you need to have a functional CUDA or ROCm environment. It can be verified by the following steps:</p> Nvidia/CUDAAMD/ROCm <p>Run the following command in a shell:</p> <pre><code>nvidia-smi | grep CUDA\n</code></pre> <p>If <code>nvidia-smi</code> does not exist, you will need to install the latest CUDA Toolkit SDK.</p> <p>Run the following command in a shell:</p> <pre><code>rocm-smi | grep rocm\n</code></pre> <p>If <code>rocm-smi</code> does not exist, you will need to install the latest ROCm Toolkit SDK.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#get-runtime-and-compiler","title":"Get runtime and compiler","text":""},{"location":"deployment-configurations/gpu-cuda-rocm/#get-iree-runtime","title":"Get IREE runtime","text":"<p>Next you will need to get an IREE runtime that includes the CUDA (for Nvidia hardware) or ROCm (for AMD hardware) HAL driver.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#build-runtime-from-source","title":"Build runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE from source, then enable the CUDA HAL driver with the <code>IREE_HAL_DRIVER_CUDA</code> option or the experimental ROCm HAL driver with the <code>IREE_EXTERNAL_HAL_DRIVERS=rocm</code> option.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#download-compiler-as-python-package","title":"Download compiler as Python package","text":"Nvidia/CUDAAMD/ROCm <p>Python packages for various IREE functionalities are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the CUDA compiler:</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable. <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre></p> <p>Currently ROCm is NOT supported for the Python interface.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#build-compiler-from-source","title":"Build compiler from source","text":"<p>Please make sure you have followed the Getting started page to build the IREE compiler, then enable the CUDA compiler target with the <code>IREE_TARGET_BACKEND_CUDA</code> option or the ROCm compiler target with the <code>IREE_TARGET_BACKEND_ROCM</code> option.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-and-run-the-model","title":"Compile and run the model","text":"<p>With the compiler and runtime ready, we can now compile a model and run it on the GPU.</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-the-model","title":"Compile the model","text":"<p>IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by main IREE compilers first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then,</p>"},{"location":"deployment-configurations/gpu-cuda-rocm/#compile-using-the-command-line","title":"Compile using the command-line","text":"<p>Let <code>iree_input.mlir</code> be the model's initial MLIR representation generated by IREE's TensorFlow importer. We can now compile them for each GPU by running the following command:</p> Nvidia/CUDAAMD/ROCm <pre><code>iree-compile \\\n--iree-hal-target-backends=cuda \\\n--iree-hal-cuda-llvm-target-arch=&lt;...&gt; \\\n--iree-hal-cuda-disable-loop-nounroll-wa \\\n--iree-input-type=mhlo \\\niree_input.mlir -o mobilenet-cuda.vmfb\n</code></pre> <p>Note that a cuda target architecture(<code>iree-hal-cuda-llvm-target-arch</code>) of the form <code>sm_&lt;arch_number&gt;</code> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to <code>sm_35</code>.</p> <p>Here are a table of commonly used architectures:</p> CUDA GPU Target Architecture Nvidia K80 <code>sm_35</code> Nvidia P100 <code>sm_60</code> Nvidia V100 <code>sm_70</code> Nvidia A100 <code>sm_80</code> <pre><code>iree-compile \\\n--iree-hal-target-backends=rocm \\\n--iree-rocm-target-chip=&lt;...&gt; \\\n--iree-rocm-link-bc=true \\\n--iree-rocm-bc-dir=&lt;...&gt; \\\n--iree-input-type=mhlo \\\niree_input.mlir -o mobilenet-rocm.vmfb\n</code></pre> <p>Note ROCm Bitcode Dir(<code>iree-rocm-bc-dir</code>) path is required. If the system you are compiling IREE in has ROCm installed, then the default value of <code>/opt/rocm/amdgcn/bitcode</code> will usually suffice. If you intend on building ROCm compiler in a non-ROCm capable system, please set <code>iree-rocm-bc-dir</code> to the absolute path where you might have saved the amdgcn bitcode.</p> <p>Note that a ROCm target chip(<code>iree-rocm-target-chip</code>) of the form <code>gfx&lt;arch_number&gt;</code> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to <code>gfx908</code> Here are a table of commonly used architecture</p> AMD GPU Target Chip AMD MI25 <code>gfx900</code> AMD MI50 <code>gfx906</code> AMD MI60 <code>gfx906</code> AMD MI100 <code>gfx908</code>"},{"location":"deployment-configurations/gpu-cuda-rocm/#run-the-model","title":"Run the model","text":""},{"location":"deployment-configurations/gpu-cuda-rocm/#run-using-the-command-line","title":"Run using the command-line","text":"<p>Run the following command:</p> Nvidia/CUDAAMD/ROCm <pre><code>iree-run-module \\\n--device=cuda \\\n--module=mobilenet-cuda.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <pre><code>iree-run-module \\\n--device=rocm \\\n--module=mobilenet-rocm.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p>"},{"location":"deployment-configurations/gpu-vulkan/","title":"Vulkan GPU HAL Driver","text":"<p>IREE can accelerate model execution on GPUs via Vulkan, a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.</p>"},{"location":"deployment-configurations/gpu-vulkan/#support-matrix","title":"Support matrix","text":"<p>As IREE and the compiler ecosystem it operates within matures, more target specific optimizations will be implemented. At this stage, expect reasonable performance across all GPUs and for improvements to be made over time for specific vendors and architectures.</p> GPU Vendor Category Performance Focus Architecture ARM Mali GPU Mobile Good Valhall Qualcomm Adreno GPU Mobile Reasonable 640+ AMD GPU Desktop/server Reasonable - NVIDIA GPU Desktop/server Reasonable -"},{"location":"deployment-configurations/gpu-vulkan/#prerequisites","title":"Prerequisites","text":"<p>In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps:</p> AndroidLinuxWindows <p>Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher.</p> <p>Run the following command in a shell:</p> <pre><code>vulkaninfo | grep apiVersion\n</code></pre> <p>If <code>vulkaninfo</code> does not exist, you will need to install the latest Vulkan SDK. For Ubuntu 18.04/20.04, installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover.</p> <p>If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.</p> <p>Run the following command in a shell:</p> <pre><code>vulkaninfo | grep apiVersion\n</code></pre> <p>If <code>vulkaninfo</code> does not exist, you will need to install the latest Vulkan SDK.</p> <p>If the showed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.</p>"},{"location":"deployment-configurations/gpu-vulkan/#get-runtime-and-compiler","title":"Get runtime and compiler","text":""},{"location":"deployment-configurations/gpu-vulkan/#get-iree-runtime-with-vulkan-hal-driver","title":"Get IREE runtime with Vulkan HAL driver","text":"<p>Next you will need to get an IREE runtime that supports the Vulkan HAL driver so it can execute the model on GPU via Vulkan.</p>"},{"location":"deployment-configurations/gpu-vulkan/#build-runtime-from-source","title":"Build runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms.</p> <p>Ensure that the <code>IREE_HAL_DRIVER_VULKAN</code> CMake option is <code>ON</code> when configuring for the target.</p>"},{"location":"deployment-configurations/gpu-vulkan/#get-compiler-for-spir-v-exchange-format","title":"Get compiler for SPIR-V exchange format","text":"<p>Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.</p>"},{"location":"deployment-configurations/gpu-vulkan/#download-as-python-package","title":"Download as Python package","text":"<p>Python packages for various IREE functionalities are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the SPIR-V compiler:</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable. <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre></p>"},{"location":"deployment-configurations/gpu-vulkan/#build-compiler-from-source","title":"Build compiler from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The SPIR-V compiler backend is compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_TARGET_BACKEND_VULKAN_SPIRV</code> CMake option is <code>ON</code> when configuring for the host.</p>"},{"location":"deployment-configurations/gpu-vulkan/#compile-and-run-the-model","title":"Compile and run the model","text":"<p>With the compiler for SPIR-V and runtime for Vulkan, we can now compile a model and run it on the GPU.</p>"},{"location":"deployment-configurations/gpu-vulkan/#compile-the-model","title":"Compile the model","text":"<p>IREE compilers transform a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by main IREE compilers first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then,</p>"},{"location":"deployment-configurations/gpu-vulkan/#compile-using-the-command-line","title":"Compile using the command-line","text":"<p>Run the following command (passing <code>--iree-input-type=</code> as needed for your import tool):</p> <p><pre><code>iree-compile \\\n--iree-hal-target-backends=vulkan-spirv \\\n--iree-vulkan-target-triple=&lt;...&gt; \\\n--iree-input-type=mhlo \\\niree_input.mlir -o mobilenet-vulkan.vmfb\n</code></pre> where <code>iree_input.mlir</code> is the imported program.</p> <p>Note that a target triple of the form <code>&lt;vendor/arch&gt;-&lt;product&gt;-&lt;os&gt;</code> is needed to compile towards each GPU architecture. If no triple is specified then a safe but more limited default will be used. We don't support the full spectrum here1; the following table summarizes the currently recognized ones:</p> GPU Vendor Target Triple ARM Mali GPU <code>valhall-g78-android30</code> Qualcomm Adreno GPU <code>adreno-unknown-android30</code> AMD GPU e.g., <code>rdna1-5700xt-linux</code> NVIDIA GPU e..g, <code>ampere-rtx3080-windows</code> SwiftShader CPU <code>cpu-swiftshader-unknown</code>"},{"location":"deployment-configurations/gpu-vulkan/#run-the-model","title":"Run the model","text":""},{"location":"deployment-configurations/gpu-vulkan/#run-using-the-command-line","title":"Run using the command-line","text":"<p>In the build directory, run the following command:</p> <pre><code>tools/iree-run-module \\\n--device=vulkan \\\n--module=mobilenet-vulkan.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p> <ol> <li> <p>It's also impossible to capture all details of a Vulkan implementation with a target triple, given the allowed variances on extensions, properties, limits, etc. So the target triple is just an approximation for usage.\u00a0\u21a9</p> </li> </ol>"},{"location":"extensions/","title":"Extension mechanisms","text":"<p>Note</p> <p>Much of this describes provisions for extension within IREE but until the core of the system has settled little work will be done to fully flesh-out and document them in detail. A large majority of things that would make someone want to extend IREE can instead be accomplished much easier and performantly using native MLIR dialects that are then processed by the IREE compiler.</p>"},{"location":"extensions/#guidelines","title":"Guidelines","text":"<p>IREE has a compiler and runtime separation, a multi-layered architecture, and split between execution of \"host code\" that schedules compute-heavy work and SPMD \"device code\" that performs the bulk of compute operations. Each axis has a different set of extension mechanisms that can be used independently or combined.</p>"},{"location":"extensions/#extension-philosophy","title":"Extension philosophy","text":"<p>Organized below are some of the mechanisms IREE provides for extending the core compiler and runtime and when they should(n't) be used. The goal of these progressively lower-level extension mechanisms is to make it easier for users to fall into the pit of success:</p> <p>Quote</p> <p>\"a well-designed system makes it easy to do the right things and annoying (but not impossible) to do the wrong things.\" - Jeff Atwood</p> <p>The amount of engineering complexity for initial bring-up and maintenance increases with each subsequently lower-level approach and it is best to start from the top and exit as fast as possible: this is a choose-your-own-adventure where you're trying to escape the dungeon with both the loot and your limbs . Avoid the temptation of immediately dropping down to making external C calls at runtime because that's how it's been done before as it's easier, more robust, and more performant to use the system as it is intended to be used.</p>"},{"location":"extensions/#when-to-extend","title":"When to extend","text":"<p>The primary goal when extending any framework should first be to avoid extending it at all. There is no mechanism that is free - whether in terms of engineering effort to develop and maintain over time, include in compiler deployments, or include in runtime deployments. As a system scales in deployment configurations the available mechanisms for extension increase but so too does the chaos introduced by extensions that do not also scale with that design. Users are the only ones who can determine the tradeoffs they are willing to accept: for example, the mechanism to extend device code with a custom runtime call to a C function does not work on GPUs and gets significantly more complicated on CPUs as sandboxes/enclaves are used - but if the user scenario is for local process CPU-only execution that may not matter.</p>"},{"location":"extensions/#where-to-extend-inputscompilerruntime","title":"Where to extend (inputs/compiler/runtime)","text":"<p>Consider in normal software development when one would choose to write more code (possibly packaging it into a reusable library) vs. changing the programming language or compiler they are using to compile their code vs. changing the operating systems their code runs on. The further one gets from the problem they are trying to solve the more work, coordination, and maintenance is involved and though there are reasons to make changes across the stack they should be done only when a simpler solution would not suffice.</p> <p>An author will retain more control over their logic the closer they sit to the inputs to the compiler. IREE provides several mechanisms that try to keep control with the author and robust to changes in IREE or MLIR internals and it is strongly encouraged that those looking to extend take those routes first. Contributions that help everyone are very welcome but do have a higher cost and it's often much easier to design and justify upstream changes with working examples in forks or at higher levels of the stack.</p>"},{"location":"extensions/#where-to-extend-hostdevice","title":"Where to extend (host/device)","text":"<p>From a performance perspective the rule is to colocate code with the data it is acting on: tensor data, for example, should almost exclusively be manipulated by device code as tensors live on device. Attempting to use tensor data with host code will result in synchronization points and host/device transfers that can decimate performance. This can lead to seemingly paradoxical situations where swapping out compiler-generated code for a human-authored \"fast path\" can be slower than even the most naive compiler results. An important thing to keep in mind with compilers is that it is exceedingly difficult to produce code by hand that is consistently more performant across a broad range of deployments and the first temptation should always be to improve the compiler - extending it via other mechanisms when not required by the task is often just premature optimization.</p>"},{"location":"extensions/#1-target-iree-input-dialects","title":"1. Target IREE input dialects","text":"<p>TL;DR</p> <p>Convert your custom ops into standard MLIR dialects.</p> <pre><code>+------------+      +--------+      +---------------+\n| Your input | -+-&gt; |  iree  | -+-&gt; | IREE compiler |\n+------------+  |   +--------+  |   +---------------+\n                |   +--------+  |\n                +-&gt; | linalg | -+\n                |   +--------+  |\n                |      ....     |\n</code></pre> <p>The easiest, cleanest, and most robust path to extend IREE is to make use of what MLIR is designed for: composing dialects and converting between them. IREE supports several input dialects such as <code>tosa</code>, <code>mhlo</code>, <code>linalg</code>, and the standard <code>arith</code>, <code>math</code>, <code>tensor</code>, and <code>scf</code> dialects. Any source IR that can be turned into that mix of dialects (directly or transitively) will work with the whole IREE pipeline for all deployment configurations and targets. If possible to express the computation in this form it will always be the best route to getting small deployments without the need to modify or include any additional code at runtime and run on all device types and execution modes.</p> <p>This mechanism can also be layered with any of the subsequent lower-level ones: if some part of the operation runs on the host and some part on device then decomposing it such that it contains as many standard ops for flow control as possible and linear algebra/custom ops for the dense math will reduce the engineering effort required on both sides and lead to an easier to maintain solution even if lower-level extension is required.</p> <p>A large majority of classic ML \"custom ops\" can be accomplished with this approach. When bringing up projects built on IREE it's best to concisely describe the operation in more elemental mathematical representations and then add optimizations where required knowing that things will still work even if those optimizations never happen.</p>"},{"location":"extensions/#pros","title":"Pros","text":"<ul> <li>No IREE compiler or runtime code changes required.<ul> <li>Can use standard IREE packaged releases and tools.</li> <li>No versioning issues at runtime.</li> </ul> </li> <li>IREE's host/device partitioning can partition your code.</li> <li>Fusion and other compiler techniques (CSE/DCE/inlining/etc) work on your code.</li> <li>All target backends (CPU/GPU/accelerators/enclaves/etc) work.</li> </ul>"},{"location":"extensions/#cons","title":"Cons","text":"<ul> <li>Input dialects cannot natively represent all possible programs (such as file   IO and other syscalls).</li> <li>Performance-sensitive host code (b-trees and other in-memory databases) will   run through the slower VM paths if not authored as dense compute.</li> </ul>"},{"location":"extensions/#when-to-use","title":"When to use","text":"<ul> <li> Targeting multiple MLIR toolchains of which IREE is just   one (as little to no IREE-specific code is required).</li> <li> Operation represents host code in addition to device code.</li> <li> All code is known statically or symbolically at   compile-time (instead of independently versioned libraries at runtime).</li> <li> Complex high-performance code not representable as linear algebra.</li> <li> External runtime interactions (file/network/user IO). Use custom modules.</li> </ul>"},{"location":"extensions/#implementation","title":"Implementation","text":"<p>To make use of this approach one just needs to follow the standard MLIR dialect conversion behavior: add a dialect with ops, add a conversion pass, and run that pass before providing the resulting IR to the IREE compiler. See Creating a Dialect.</p> <p>Think of this like authoring C++ sources with templates that you compile into your application: Clang (and LLVM beyond) don't know about your library details and instead just process it as it would any other code. You can take the same source and pass it to GCC and it'll be robust to underlying changes in the system.</p>"},{"location":"extensions/#2-extend-host-code-with-custom-modules","title":"2. Extend host code with custom modules","text":"<p>TL;DR</p> <p>Import MLIR functions in the compiler and custom modules at runtime.</p> <pre><code>// Main user module compiled by IREE:\nmodule @model {\n  // Declare a synchronous external function:\n  func.func private @my_custom_module.sync_func(%input: tensor&lt;?xf32&gt;) -&gt; i32\n  // Declare an asynchronous external function:\n  func.func private @my_custom_module.async_func(%input: tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt; attributes {\n    iree.abi.model = \"coarse-fences\",\n    nosideeffects\n  }\n  func.func @predict() {\n    ...\n    // Call a synchronous/blocking external function:\n    %sync_result = call @my_custom_module.sync_func(%sync_input) : (tensor&lt;?xf32&gt;) -&gt; i32\n    ...\n    ...\n    // Call an asynchronous/non-blocking external function:\n    %async_result = call @my_custom_module.async_func(%async_input) : (tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt;\n    ...\n  }\n}\n</code></pre> <p>IREE provides dynamic linking at runtime via its VM interfaces. For code that runs on the host and requires syscalls or calling out to existing libraries - such as file IO, text processing, and JPEG decoding - this is an easy way to interop without paying attention to the more complex details of device code. An IREE module compiled using custom modules is portable and dynamically deployable so long as the custom module is registered at runtime.</p> <p>This approach conceptually matches what normal native binaries do in an OS: imports are declared and at runtime they are resolved based on the available exports of modules in the system. Just as with normal systems engineering design of the API between modules is up to the user and depending on rigor can have several pitfalls but these problems and their solutions are not IREE specific and anyone who has designed a shared library interface can apply the same rules here in IREE around versioning, performance, etc. One does not add 2 integers via a syscall and the same holds here: custom modules and the functions within should perform a large amount of work to hide overheads involved in the cross-module calls and users must be aware that the compiler cannot optimize across the call boundaries.</p> <p>See the synchronous tensor I/O and asynchronous tensor I/O samples.</p>"},{"location":"extensions/#pros_1","title":"Pros","text":"<ul> <li>No IREE compiler code changes required.</li> <li>Produced artifacts are portable across IREE deployment configurations.</li> <li>Full system access is allowed - the VM just calls external functions.</li> <li>Runtime modules can be implemented (via shims) in other languages/runtimes.</li> </ul>"},{"location":"extensions/#cons_1","title":"Cons","text":"<ul> <li>Custom modules must be registered at runtime by the user.</li> <li>The VM custom module ABI goo must be authored by the user (such as with JNI or   pybind to move between java/python and C).</li> <li>All custom module code must be compiled and deployed regardless of how much   any modules use. The granularity of modules and their versioning is up to the   user.</li> <li>Custom module code cannot be optimized by the IREE compiler to avoid   host/device readbacks and unnecessary data type conversion.</li> </ul>"},{"location":"extensions/#when-to-use_1","title":"When to use","text":"<ul> <li> Interactions with large libraries or system calls.</li> <li> Performance-sensitive host code that cannot easily be   represented as device code (like UTF-8 string transformation using libicu).</li> <li> Extensively using tensor resources.</li> </ul>"},{"location":"extensions/#implementation_1","title":"Implementation","text":"<p>The runtime portion requires that the code be exported to the VM system by way of an <code>iree_vm_module_t</code> interface. A low-level native interface exists with minimal overhead and is used for example by the IREE HAL itself. There is also a C++ wrapper that is significantly easier to work with however it needs some performance improvements.</p> <p>Full end-to-end examples can be found under <code>samples/custom_modules/</code>:</p> <ul> <li>The basic sample shows how to add VM modules with custom types and take advantage of ABI features like fallback functions and optional imports.</li> <li>The synchronous tensor I/O sample shows a call taking and returning a tensor and performing blocking work.</li> <li>The asynchronous tensor I/O sample shows the same thing but with fences for asynchronous scheduling.</li> </ul>"},{"location":"extensions/#3-extend-target-specific-device-conversion-patterns","title":"3. Extend target-specific device conversion patterns","text":"<p>TL;DR</p> <p>Add patterns to <code>iree/Compiler/Codegen/</code> to emit target code.</p> <p>The easiest and most robust path for specializations of device code is to emit such code mixed with the IREE compiler generated code at the highest possible level of abstraction within the target pipeline. For example, if the code can be represented with the <code>vector</code> dialect then inserting conversion patterns between <code>linalg</code> and <code>vector</code> enables the emitted code to be specialized further based on user configuration and optimized with the full set of available passes that run in the pipeline. For each level lower one goes the more flexibility they gain such as being able to emit inline assembly blocks that do anything while trading off generality and multi-targeting applicability.</p> <p>How much the tradeoff matters is based on the behavior of the extension. If a pattern changing a transcendental function to an approximation can operate at the vector level then all IREE deployment targets can benefit from the pattern and as new targets are made available they will automatically receive the benefits. In contrast, a pattern at the vector level that turns generic vector operations into architecture-specific LLVM intrinsics by its nature only pertains to a single target family and can be done at a lower level. As a rule of thumb if a particular pattern is going to need ~N implementations for ~N targets that are all mostly the same it's better to try to move that higher in the stack.</p> <p>At this point the complexity of extending things is still fairly constrained: a C++ pass or pattern is verified with normal lit tests and can be upstreamed easily either into MLIR or IREE (a large number of IREE patterns are upstreamed, benefiting all users of MLIR). Cross-compilation and versioning are not a factor and the IREE artifacts can be considered durable at a coarse level (outside of major target architectural changes).</p> <p>Note that depending on the target there are various mechanisms for representing code in MLIR, up to including inline assembly snippets in IR via <code>llvm.inline_asm</code>.</p>"},{"location":"extensions/#pros_2","title":"Pros","text":"<ul> <li>Not limited to what is possible to represent in any particular MLIR dialect.</li> <li>Rich target configuration available; multiple passes can contribute info.</li> <li>Produced executable binaries are hermetic and no runtime changes are required.</li> <li>Specialization can happen in MLIR dialects like <code>linalg</code> or <code>vector</code> as well   as target-specific representations like SPIR-V and LLVM IR.</li> <li>The compiler can perform deep optimizations across both the generated code and   the provided code (hoisting/loop invariant code motion/cse/etc).</li> </ul>"},{"location":"extensions/#cons_2","title":"Cons","text":"<ul> <li>Requires implementing the patterns as code in the IREE compiler or via TBD   interfaces.</li> </ul>"},{"location":"extensions/#when-to-use_2","title":"When to use","text":"<ul> <li> Code that must be emitted during target lowering - such as   something optimizing for a particular CPU architecture.</li> <li> Hot code mixed with generated code at a fine granularity   (within the innermost loop).</li> <li> External existing hand-authored libraries. Either statically   or dynamically link instead.</li> </ul>"},{"location":"extensions/#implementation_2","title":"Implementation","text":"<p>There are several ways to author patterns and passes in MLIR. As examples:</p> <ul> <li>A majority of patterns are authored in C++ using PatternRewriter.</li> <li>PDL is an MLIR-based way to   express rewrite operations with strong typing, compile-time verification, and   easily-readable and less-verbose IR.</li> <li><code>linalg</code> uses a python-based DSL   for defining some of its extended ops.</li> </ul> <p>There are many examples within both MLIR and IREE, one specifically being the polynomial approximation expansion patterns.</p>"},{"location":"extensions/#4-include-external-target-specific-device-code","title":"4. Include external target-specific device code","text":"<p>TL;DR</p> <p>Statically link external object files into IREE executables.</p> <p>For large bodies of existing device code or library calls that are available for static linkage the work involved to reimplement them at higher levels of the stack can be cost prohibitive even if it leads to better results. In these cases just as with a normal toolchain one would just want to declare an external function, call it, and add the object file to the linker command line. In IREE the same can be performed by way of taking compatible bitcode or native object files and linking them in with the generated code. An MLIR pattern would declare and emit the call and the target-specific IREE linker would pull in the objects.</p> <p>As the linking behavior varies per target (for example, some targets like SPIR-V don't have traditional linkers) how this is performed is up to the IREE target backends. The complexity involved in producing the object files to link will also vary per-backend and the complexity of the deployment: cross-compiling for multiple architectures or compilation modes (ASAN, etc) will require unique copies of the object files matching that precise configuration.</p> <p>At this point generality is largely out as is the ability to cleanly upstream such files. It should be apparent how a few dozen lines of C++ or PDL that avoids the need for any of this complexity is more appealing. In extremely specific cases of a single platform/architecture/version for a single program deployed via a specific artifact composition it's not so bad but IREE is designed such that extreme specificity is an optional mode of the more general solution. This does not mean this mechanism is not useful in some situations and only that it should be a last-resort when one of the easier to manage solutions is not viable - not a shortcut to avoid writing some C++ patterns.</p>"},{"location":"extensions/#pros_3","title":"Pros","text":"<ul> <li>Works with hand-authored code in compatible object files from any toolchain.</li> <li>No IREE runtime changes required.<ul> <li>All deployment modes still work, including multi-targeting.</li> <li>No versioning concerns as custom code is included in artifacts.</li> </ul> </li> </ul>"},{"location":"extensions/#cons_3","title":"Cons","text":"<ul> <li>Users must provide per-target precompiled object files on disk.</li> <li>IREE compiler changes are still needed for generating the external calls.</li> <li>Though LTO may be able to optimize across the calls it is not guaranteed.</li> </ul>"},{"location":"extensions/#when-to-use_3","title":"When to use","text":"<ul> <li> Existing math libraries or architecture-specific functions   that cannot be ported into a more MLIR-friendly form.</li> <li> Mixing in hand-authored code written in C/rust/etc with   generated code from MLIR.</li> <li> External code can be represented as either <code>linalg</code>,   <code>vector</code>, or LLVM IR. Use target-specific conversion patterns instead.</li> <li> External code size is large and unlikely to benefit from   link-time optimizations (such as something like libjpeg). Dynamically link   instead.</li> </ul>"},{"location":"extensions/#implementation_3","title":"Implementation","text":"<p>As the linking behavior varies per target backend there is no general solution at this level: if targeting the CPU then the system native linker or lld need to be provided the object files, while SPIR-V will need to merge the SPIR-V binaries directly, and Metal shader libraries will need to be constructed with the Apple-specific <code>metallib</code> tooling. Producing these files and performing the linking is outside the scope of IREE.</p> <p>If the files can be acquired then compiler changes will be required to emit calls to them and invoke the linker with the the files.</p> <p>On the CPU an alternative is to use the static library output mode where IREE produces an object file and then the user invokes the linker themselves; this still requires the compiler changes to emit the calls but avoids needing to teach the compiler how to link the files.</p>"},{"location":"extensions/#5-dynamically-link-target-specific-device-code-cpu-only","title":"5. Dynamically link target-specific device code (CPU only)","text":"<p>TL;DR</p> <p>Dynamically link external C functions at runtime from device code.</p> <p>It is pitch black. You are likely to be eaten by a grue.</p> <p>This is the lowest-level integration in the system and is designed to act as an escape hatch and - as with any emergency escape hatch - it's not designed for ergonomics. Users should try first to come in through the door and attempting to use this mechanism should trigger alarms about the approach being attempted.</p> <p>IREE's execution model for device code and native machine binary deployment mechanisms are designed with several constraints in order to make all of the above approaches possible and performant. Calling arbitrary C functions from deep within the system can introduce subtle (and not-so-subtle) bugs that are extremely difficult to track down and versioning between the compiler emitting the calls and the runtime providing the implementations can cause skew unless held carefully. Consider the methods added here like syscalls in that they must be extremely focused and if they are ever likely to change (including being removed) then care will be needed just as with versioning or redirecting a syscall. Designing good stable interfaces is hard and a classic pit of failure.</p> <p>Some things to note:</p> <ul> <li>Device code executes in a tiled fashion and single dispatches may invoke the   same function many times from many threads concurrently to perform   the larger work.</li> <li>Tiles may execute in any order and on any thread; performing fine-grained   locking within the tile can lead to deadlocks.</li> <li>Device code is stateless in order to allow for access restrictions and caching   across multiple loaded models - any library state required must be externally   managed via process globals.</li> <li>Device code may be running out-of-process (sandbox/enclave) and the library   functions must be available where the dispatches run and not where they are   launched (such as being linked into the sandbox binary, if separate from the   main process binary).</li> <li>The stack must be used to pass arguments/results to external calls via a   single pointer and there is no libffi-like functionality for magically calling   arbitrary C functions. Users must provide the shims they need.</li> <li>Thread-local storage is unavailable in the called code (it may be usable, but   it is not guaranteed it'll work on all platforms and leaks are likely).</li> <li>No heap allocator is provided and the use of libc malloc is unsupported.</li> </ul> <p>Most of the constraints here come from the SPMD parallelism model, platform-agnostic deployment format, and overall data-oriented design of IREE. Code operating in this fashion has a certain shape and that is usually not the same as big legacy single-threaded CPU-focused BLAS libraries that perform their own caching, internal thread and state management, and other shenanigans. IREE is not designed to wrap such things and if any of these notes are issues it is more an indicator that the approach needs adjustment than anything else. Trying to bypass or workaround the constraints is possible - after all IREE is an open source project and any user is welcome to fork it - but unsupported by the core IREE team.</p>"},{"location":"extensions/#pros_4","title":"Pros","text":"<ul> <li>Function resolution at runtime is orthogonal to compiler target specification.</li> <li>Machine code can be shared between the application and IREE artifacts.</li> </ul>"},{"location":"extensions/#cons_4","title":"Cons","text":"<ul> <li>IREE compiler and runtime must both be modified.</li> <li>Deeper integration with the IREE codegen compiler infrastructure required.</li> <li>ABI versioning complexity between compiler and runtime.</li> <li>Runtimes must ship the imports for the lifetime of any artifact compiled to   use them.<ul> <li>Humans are bad at predicting the future.</li> <li>Using the same artifact in different binaries at runtime requires changes   to each binary - including those that may not be owned by the person   producing the artifact.</li> <li>Weak imports and conditional usage can help but still leads to bloat.</li> </ul> </li> </ul>"},{"location":"extensions/#when-to-use_4","title":"When to use","text":"<ul> <li> Calling into opaque closed-source BLAS-like microkernel   libraries.</li> <li> Any other cases covered above can be used, especially   microkernels that can be represented in MLIR or as statically linked   libraries.</li> </ul>"},{"location":"extensions/#implementation_4","title":"Implementation","text":"<p>The compiler is changed to produce calls to imports via a dynamic import table provided to each dispatch function. The import table is declared in the executable library for use at runtime. Runtime applications register an import provider to resolve named symbols in the import table to C functions that marshal arguments and results.</p> <p>The compiler-side needs some additional work but an example is included here: Issue 7504. The runtime-side is complete and resolution is performed by a user-supplied <code>iree_hal_executable_import_provider_t</code>.</p>"},{"location":"getting-started/","title":"Getting Started Guide","text":""},{"location":"getting-started/#setup","title":"Setup","text":"<p>Use the following command for the default installation, or check out the comprehensive installation guide if your needs are more complex.</p> <pre><code>python -m pip install \\\n  iree-compiler \\\n  iree-runtime \\\n  iree-tools-tf \\\n  iree-tools-tflite \\\n  iree-tools-xla\n</code></pre>"},{"location":"getting-started/#supported-frameworks","title":"Supported frameworks","text":"<p>See end-to-end examples of how to execute a variety models on IREE. This covers the import, compilation, and execution of the provided model.</p> <ul> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>JAX</li> <li>PyTorch</li> </ul> <p>Importing from other frameworks is planned - stay tuned!</p>"},{"location":"getting-started/#samples","title":"Samples","text":"<p>Check out the samples in IREE's samples/colab/ directory, as well as the iree-samples repository, which contains workflow comparisons across frameworks.</p>"},{"location":"getting-started/#import","title":"Import","text":"<p>Importing models takes known file types and imports into a form that the core IREE compiler is able to ingest. This import process is specific to each frontend and typically involves a number of stages:</p> <ul> <li>Load the source format</li> <li>Legalize operations specific each specific frontend to legal IR</li> <li>Validate only IREE compatible operations remain</li> <li>Write the remaining IR to a file</li> </ul> <p>This fully legalized form can then be compiled without dependencies on the source model language.</p>"},{"location":"getting-started/#compilation","title":"Compilation","text":"<p>During compilation we load an MLIR file and compile for the specified set of backends (CPU, GPU, etc).  Each of these backends creates custom native code to execute on the target device.  Once compiled, the resulting bytecode is exported to an IREE bytecode file that can be executed on the specified devices.</p>"},{"location":"getting-started/#execution","title":"Execution","text":"<p>The final stage is executing the now compiled module. This involves selecting what compute devices should be used, loading the module, and executing the module with the intended inputs. For testing, IREE includes a Python API. However, on mobile and embedded devices you will want to use the C API.</p>"},{"location":"getting-started/jax/","title":"JAX Integration","text":"<p>Note</p> <p>IREE's JAX support is under active development. This page is still under construction.</p> <p>IREE offers two ways to interface with JAX programs:</p> <ul> <li>An API for extracting and compiling full models ahead of time (AOT) for   execution apart from JAX. This API is being developed in the   iree-org/iree-jax repository.</li> <li>A PJRT plugin that adapts IREE as a native JAX backend for online / just in   time (JIT) use. This plugin is being developed in the   pjrt-plugin/   folder within the iree-samples repository for now, though it will likely move   elsewhere as it matures.</li> </ul>"},{"location":"getting-started/pytorch/","title":"PyTorch Integration","text":"<p>IREE supports compiling and running PyTorch programs represented as <code>nn.Module</code> classes as well as models defined using <code>functorch</code>.</p>"},{"location":"getting-started/pytorch/#prerequisites","title":"Prerequisites","text":"<p>Install IREE pip packages, either from pip or by building from source:</p> <pre><code>pip install \\\niree-compiler \\\niree-runtime\n</code></pre> <p>Install <code>torch-mlir</code>, necessary for compiling PyTorch models to a format IREE is able to execute:</p> <pre><code>pip install -f https://llvm.github.io/torch-mlir/package-index/ torch-mlir\n</code></pre> <p>A special <code>iree_torch</code> package makes it easy to compile PyTorch programs and run them on IREE:</p> <pre><code>pip install git+https://github.com/iree-org/iree-torch.git\n</code></pre>"},{"location":"getting-started/pytorch/#running-a-model","title":"Running a model","text":"<p>Going from a loaded PyTorch model to one that's executing on IREE happens in four steps:</p> <ol> <li>Compile the model to MLIR</li> <li>Compile the MLIR to IREE VM flatbuffer</li> <li>Load the VM flatbuffer into IREE</li> <li>Execute the model via IREE</li> </ol> <p>Note</p> <p>In the following steps, we'll be borrowing the model from this BERT colab and assuming it is available as <code>model</code>.</p>"},{"location":"getting-started/pytorch/#compile-the-model-to-mlir","title":"Compile the model to MLIR","text":"<p>First, we need to trace and compile our model to MLIR:</p> <pre><code>model = # ... the model we're compiling\nexample_input = # ... an input to the model with the expected shape and dtype\nmlir = torch_mlir.compile(\n    model,\n    example_input,\n    output_type=torch_mlir.OutputType.LINALG_ON_TENSORS,\n    use_tracing=True)\n</code></pre> <p>The full list of available output types can be found here and includes linalg on tensors, mhlo, and tosa.</p>"},{"location":"getting-started/pytorch/#compile-the-mlir-to-an-iree-vm-flatbuffer","title":"Compile the MLIR to an IREE VM flatbuffer","text":"<p>Next, we compile the resulting MLIR to IREE's deployable file format:</p> <pre><code>iree_backend = \"llvm-cpu\"\niree_vmfb = iree_torch.compile_to_vmfb(mlir, iree_backend)\n</code></pre> <p>Here we have a choice of backend we want to target. See the Deployment Configurations section of this site for a full list of targets and configurations.</p> <p>The generated flatbuffer can now be serialized and stored for another time or loaded and executed immediately.</p>"},{"location":"getting-started/pytorch/#load-the-vm-flatbuffer-into-iree","title":"Load the VM flatbuffer into IREE","text":"<p>Next, we load the flatbuffer into the IREE runtime. <code>iree_torch</code> provides a convenience method for loading this flatbuffer from Python:</p> <pre><code>invoker = iree_torch.load_vmfb(iree_vmfb, iree_backend)\n</code></pre>"},{"location":"getting-started/pytorch/#execute-the-model-via-iree","title":"Execute the model via IREE","text":"<p>Finally, we can execute the loaded model:</p> <pre><code>result = invoker.forward(example_input)\n</code></pre>"},{"location":"getting-started/pytorch/#training","title":"Training","text":"<p>Training with PyTorch in IREE is supported via <code>functorch</code>. The steps for loading the model into IREE, once defined, are nearly identical to the above example.</p> <p>You can find a full end-to-end example of defining a basic regression model, training with it, and running inference on it here.</p>"},{"location":"getting-started/pytorch/#native-on-device-training","title":"Native / On-device Training","text":"<p>A small (~100-250KB), self-contained binary can be built for deploying to resource-constrained environments. An example illustrating this can be found in this example. This binary runs a model without a Python interpreter.</p>"},{"location":"getting-started/pytorch/#samples","title":"Samples","text":"Colab notebooks Inference on BERT Example scripts Basic Inference and Training Example Native On-device Training Example"},{"location":"getting-started/tensorflow/","title":"TensorFlow Integration","text":"<p>IREE supports compiling and running TensorFlow programs represented as <code>tf.Module</code> classes or stored in the <code>SavedModel</code> format.</p>"},{"location":"getting-started/tensorflow/#prerequisites","title":"Prerequisites","text":"<p>Install TensorFlow by following the official documentation:</p> <pre><code>python -m pip install tf-nightly\n</code></pre> <p>Install IREE pip packages, either from pip or by building from source:</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime \\\niree-tools-tf\n</code></pre> <p>Warning</p> <p>The TensorFlow package is currently only available on Linux and macOS. It is not available on Windows yet (see this issue).</p>"},{"location":"getting-started/tensorflow/#importing-models","title":"Importing models","text":"<p>IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the <code>iree-import-tf</code> command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR) compatible with the generic IREE compilers.</p>"},{"location":"getting-started/tensorflow/#from-savedmodel-on-tensorflow-hub","title":"From SavedModel on TensorFlow Hub","text":"<p>IREE supports importing and using SavedModels from TensorFlow Hub.</p>"},{"location":"getting-started/tensorflow/#using-the-command-line-tool","title":"Using the command-line tool","text":"<p>First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow:</p> <pre><code>import tensorflow.compat.v2 as tf\nloaded_model = tf.saved_model.load('/path/to/downloaded/model/')\nprint(list(loaded_model.signatures.keys()))\n</code></pre> <p>Note</p> <p>If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\".</p> <p>Then you can import the model with <code>iree-import-tf</code>. You can read the options supported via <code>iree-import-tf -help</code>. Using MobileNet v2 as an example and assuming the serving signature is <code>predict</code>:</p> <pre><code>iree-import-tf\n  --tf-import-type=savedmodel_v1 \\\n--tf-savedmodel-exported-names=predict \\\n/path/to/savedmodel -o iree_input.mlir\n</code></pre> <p>Tip</p> <p><code>iree-import-tf</code> is installed as <code>/path/to/python/site-packages/iree/tools/tf/iree-import-tf</code>. You can find out the full path to the <code>site-packages</code> directory via the <code>python -m site</code> command.</p> <p><code>-tf-import-type</code> needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump.</p> <p>Afterwards you can further compile the model in <code>iree_input.mlir</code> for CPU or GPU.</p>"},{"location":"getting-started/tensorflow/#training","title":"Training","text":"<p>Todo</p> <p>Discuss training</p>"},{"location":"getting-started/tensorflow/#samples","title":"Samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection module Pretrained ResNet50 inference TensorFlow Hub Import <p>End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.</p>"},{"location":"getting-started/tensorflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/tensorflow/#missing-serving-signature-in-savedmodel","title":"Missing serving signature in SavedModel","text":"<p>Sometimes SavedModels are exported without explicit serving signatures. This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2, assuming we want the serving signature to be <code>predict</code> and operating on a 224x224 RGB image:</p> <pre><code>import tensorflow.compat.v2 as tf\nloaded_model = tf.saved_model.load('/path/to/downloaded/model/')\ncall = loaded_model.__call__.get_concrete_function(\n         tf.TensorSpec([1, 224, 224, 3], tf.float32))\nsignatures = {'predict': call}\ntf.saved_model.save(loaded_model,\n  '/path/to/resaved/model/', signatures=signatures)\n</code></pre> <p>The above will create a new SavedModel with a serving signature, <code>predict</code>, and save it to <code>/path/to/resaved/model/</code>.</p>"},{"location":"getting-started/tflite/","title":"TFLite Integration","text":"<p>IREE supports compiling and running TensorFlow Lite programs stored as TFLite FlatBuffers. These files can be imported into an IREE-compatible format then compiled to a series of backends.</p>"},{"location":"getting-started/tflite/#prerequisites","title":"Prerequisites","text":"<p>Install TensorFlow-Lite specific dependencies using pip:</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime \\\niree-tools-tflite\n</code></pre>"},{"location":"getting-started/tflite/#importing-and-compiling","title":"Importing and Compiling","text":"<p>IREE's tooling is divided into two components: import and compilation.</p> <ol> <li>The import tool converts the TFLite FlatBuffer to an IREE compatible form, validating that only IREE compatible operations remain. Containing a combination of TOSA and IREE operations.</li> <li>The compilation stage generates the bytecode module for a list of targets, which can be executed by IREE.</li> </ol>"},{"location":"getting-started/tflite/#using-command-line-tools","title":"Using Command Line Tools","text":"<p>These two stages can be completed entirely via the command line.</p> <pre><code>WORKDIR=\"/tmp/workdir\"\nTFLITE_URL=\"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\"\nTFLITE_PATH=${WORKDIR}/model.tflite\nIMPORT_PATH=${WORKDIR}/tosa.mlir\nMODULE_PATH=${WORKDIR}/module.vmfb\n\n# Fetch the sample model\nwget ${TFLITE_URL} -O ${TFLITE_PATH}\n\n# Import the sample model to an IREE compatible form\niree-import-tflite ${TFLITE_PATH} -o ${IMPORT_PATH}\n\n# Compile for the CPU backend\niree-compile \\\n--iree-input-type=tosa \\\n--iree-hal-target-backends=llvm-cpu \\\n${IMPORT_PATH} \\\n-o ${MODULE_PATH}\n</code></pre>"},{"location":"getting-started/tflite/#using-the-python-api","title":"Using the Python API","text":"<p>The example below demonstrates downloading, compiling, and executing a TFLite model using the Python API. This includes some initial setup to declare global variables, download the sample module, and download the sample inputs.</p> <p>Declaration of absolute paths for the sample repo and import all required libraries. The default setup uses the CPU backend as the only target. This can be reconfigured to select alternative targets.</p> <pre><code>import iree.compiler.tflite as iree_tflite_compile\nimport iree.runtime as iree_rt\nimport numpy\nimport os\nimport urllib.request\n\nfrom PIL import Image\n\nworkdir = \"/tmp/workdir\"\nos.makedirs(workdir, exist_ok=True)\n\ntfliteFile = \"/\".join([workdir, \"model.tflite\"])\njpgFile = \"/\".join([workdir, \"input.jpg\"])\ntfliteIR = \"/\".join([workdir, \"tflite.mlir\"])\ntosaIR = \"/\".join([workdir, \"tosa.mlir\"])\nbytecodeModule = \"/\".join([workdir, \"iree.vmfb\"])\n\nbackends = [\"llvm-cpu\"]\nconfig = \"local-task\"\n</code></pre> <p>The TFLite sample model and input are downloaded locally.</p> <pre><code>tfliteUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\"\njpgUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8_input.jpg\"\n\nurllib.request.urlretrieve(tfliteUrl, tfliteFile)\nurllib.request.urlretrieve(jpgUrl, jpgFile)\n</code></pre> <p>Once downloaded we can compile the model for the selected backends. Both the TFLite and TOSA representations of the model are saved for debugging purposes. This is optional and can be omitted.</p> <pre><code>iree_tflite_compile.compile_file(\n  tfliteFile,\n  input_type=\"tosa\",\n  output_file=bytecodeModule,\n  save_temp_tfl_input=tfliteIR,\n  save_temp_iree_input=tosaIR,\n  target_backends=backends,\n  import_only=False)\n</code></pre> <p>After compilation is completed we configure the VmModule using the local-task configuration and compiled IREE module.</p> <pre><code>config = iree_rt.Config(\"local-task\")\ncontext = iree_rt.SystemContext(config=config)\nwith open(bytecodeModule, 'rb') as f:\n  vm_module = iree_rt.VmModule.from_flatbuffer(config.vm_instance, f.read())\n  context.add_vm_module(vm_module)\n</code></pre> <p>Finally, the IREE module is loaded and ready for execution. Here we load the sample image, manipulate to the expected input size, and execute the module. By default TFLite models include a single function named 'main'. The final results are printed.</p> <pre><code>im = numpy.array(Image.open(jpgFile).resize((192, 192))).reshape((1, 192, 192, 3))\nargs = [im]\n\ninvoke = context.modules.module[\"main\"]\niree_results = invoke(*args)\nprint(iree_results)\n</code></pre>"},{"location":"getting-started/tflite/#troubleshooting","title":"Troubleshooting","text":"<p>Failures during the import step usually indicate a failure to lower from TensorFlow Lite's operations to TOSA, the intermediate representation used by IREE. Many TensorFlow Lite operations are not fully supported, particularly those than use dynamic shapes. File an issue to IREE's TFLite model support project.</p>"},{"location":"getting-started/tflite/#additional-samples","title":"Additional Samples","text":"<ul> <li> <p>The tflitehub folder in the iree-samples repository contains test scripts to compile, run, and compare various TensorFlow Lite models sourced from TensorFlow Hub.</p> </li> <li> <p>An example smoke test of the TensorFlow Lite C API is available here.</p> </li> </ul> Colab notebooks Text classification with TFLite and IREE <p>Todo</p> <p>Issue#3954: Add documentation for an Android demo using the Java TFLite bindings, once it is complete at not-jenni/iree-android-tflite-demo.</p>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>IREE exists in an ecosystem of projects, each using their own terminology and interacting in various ways. Below is a summation of these projects with the problems they are built to address.</p>"},{"location":"reference/glossary/#jax","title":"JAX","text":"<p>JAX is a front-end for writing and executing Machine Learning models, including support for Google Cloud TPUs.</p>"},{"location":"reference/glossary/#mlir","title":"MLIR","text":"<p>Multi Level Intermediate Representation is the compiler framework that IREE is built around. Beyond the tooling this includes a set of common dialects and transformations that IREE utilizes for its code generation system.</p> <p>For general discussion on MLIR see the project's discourse group.</p>"},{"location":"reference/glossary/#linalg","title":"LinAlg","text":"<p>Linalg is an MLIR dialect that defines how Linear Algebra operations can be described in a generalized fashion, including a set of commonly used operations. IREE's code generation defines tensor operations using the Linalg dialect, then uses it to generate the loop structures for the CPU and GPU backends.</p>"},{"location":"reference/glossary/#spir-v","title":"SPIR-V","text":"<p>SPIR-V is a shader and kernel intermediate language for expressing parallel computation typically used for GPUs. It serves as a hardware agnostic assembly format for distributing complex, computationally intensive programs. It is the preferred method for shipping platform agnostic binaries to run on GPUs.</p>"},{"location":"reference/glossary/#tosa","title":"TOSA","text":"<p>The TOSA specification defines a set of common tensor operations to most machine learning frameworks. This simplifies model compilation as separate front-end frameworks can target TOSA's intermediate representation without compromising on the ability to achieve efficient execution across multiple device types.</p> <p>IREE uses the TOSA MLIR dialect as a prioritized ingestion format, transforming multiple ML-platform ingestion formats into a TOSA compatible set of operations.</p> <p>Changes to the TOSA specification require submitting a proposal on TOSA's platform development page</p>"},{"location":"reference/glossary/#tflite","title":"TFLite","text":"<p>TensorFlow lite is a model format and execution system for performing on device inference. IREE supports TFLite FlatBuffers translation into TOSA operations and compilation into the LinAlg dialect.</p>"},{"location":"reference/optimization-options/","title":"Optimization Options","text":"<p>This page documents various supported flags for optimizing IREE programs. Each is presented with its English name, flag to enable/disable, and default state.</p> <p>These flags can be passed to the:</p> <ul> <li><code>iree-compile</code> command line tool</li> <li><code>extra_args=[\"--flag\"]</code> argument to <code>iree.compiler.tools</code> Python wrappers</li> <li>In-process Python compiler API   <code>iree.compiler.transforms.iree-compile.CompilerOptions(\"--flag\", \"--flag2\")</code>   constructor</li> <li><code>ireeCompilerOptionsSetFlags()</code> compiler C API function</li> </ul>"},{"location":"reference/optimization-options/#high-level-program-optimizations","title":"High level program optimizations","text":""},{"location":"reference/optimization-options/#constant-evaluation-iree-opt-const-eval-off","title":"Constant evaluation (<code>--iree-opt-const-eval</code> (off))","text":"<p>Performs compile-time evaluation of any global initializers which produce the initial values for global constants, storing the global directly in the program as constant data. This extracts such constant program fragments and recursively compiles them, using the runtime to evaluate the results.</p> <p>Note that this only has any effect on computations in module initializer functions, not free-standing operations in the program which may produce constant-derived results. See <code>--iree-opt-const-expr-hoisting</code> for options to optimize these.</p>"},{"location":"reference/optimization-options/#constant-expression-hoisting-iree-opt-const-expr-hoisting-off","title":"Constant expression hoisting (<code>--iree-opt-const-expr-hoisting</code> (off))","text":"<p>Identifies all trees of constant expressions in the program and uses a heuristic to determine which would be profitable to hoist into global initializers for evaluation at module load. Together with <code>--iree-opt-const-eval</code>, this will convert eligible trees of expressions to purely static data embedded in the module.</p> <p>The heuristic is currently relatively primitive, using static information to disable hoisting of leaf operations which are metadata only (i.e. broadcasts, etc) or are expected to fold away as part of operator fusion. Notably, the current heuristic is likely to pessimize module size in the case of complicated programs with trees of constant, large tensors.</p>"},{"location":"reference/optimization-options/#numeric-precision-reduction-iree-opt-numeric-precision-reduction-off","title":"Numeric precision reduction (<code>--iree-opt-numeric-precision-reduction</code> (off))","text":"<p>Analyzes program constant data and program flow to identify math operations which can be safely evaluated with reduced precision (currently with a minimum of 8bit integers but being extended to infer any bit depth) and inserts appropriate casts. In conjunction with Constant Expression Hoisting, Constant Evaluation and other automatic optimizations, this can produce programs where large amounts (up to the whole) have had their numeric operations and constant data rewritten to lower precision types.</p> <p>This feature is actively evolving and will be the subject of dedicated documentation when ready.</p>"},{"location":"reference/optimization-options/#strip-debug-assertions-iree-opt-strip-assertions-off","title":"Strip Debug Assertions (<code>--iree-opt-strip-assertions</code> (off))","text":"<p>Strips all <code>std.assert</code> ops in the input program after useful information for optimization analysis has been extracted. Assertions provide useful user-visible error messages but can prevent critical optimizations. Assertions are not, however, a substitution for control flow and frontends that want to check errors in optimized release builds should do so via actual code - similar to when one would <code>if (foo) return false;</code> vs. <code>assert(foo);</code> in a normal program.</p>"}]}